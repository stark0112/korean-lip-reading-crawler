#!/usr/bin/env python3
"""
VALLR í•œêµ­ì–´ íŠ¹í™” ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
"""

import os
import json
import argparse
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import torch
from torch.utils.data import DataLoader

# í•œêµ­ì–´ íŠ¹í™” ëª¨ë“ˆë“¤
from src.korean_grapheme_processor import KoreanGraphemeProcessor
from src.korean_data_loader import KoreanLipReadingDataset, create_korean_dataloader

class KoreanVALLRPreprocessor:
    """í•œêµ­ì–´ VALLR ì „ì²˜ë¦¬ê¸°"""
    
    def __init__(self, 
                 data_root: str,
                 output_dir: str = "processed_data",
                 max_video_length: int = 768,  # VALLR ë…¼ë¬¸: ViT ìµœëŒ€ ì…ë ¥ ê¸¸ì´
                 max_text_length: int = 300,
                 lip_size: Tuple[int, int] = (224, 224),  # VALLR ë…¼ë¬¸ ê¸°ì¤€
                 use_audio: bool = False,
                 use_visual: bool = True):
        """
        Args:
            data_root: ì›ë³¸ ë°ì´í„° ë£¨íŠ¸ ê²½ë¡œ
            output_dir: ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ê²½ë¡œ
            max_video_length: ìµœëŒ€ ë¹„ë””ì˜¤ ê¸¸ì´
            max_text_length: ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´
            lip_size: ë¦½ ì´ë¯¸ì§€ í¬ê¸°
            use_audio: ì˜¤ë””ì˜¤ ì‚¬ìš© ì—¬ë¶€
            use_visual: ë¹„ì£¼ì–¼ ì‚¬ìš© ì—¬ë¶€
        """
        self.data_root = Path(data_root)
        self.output_dir = Path(output_dir)
        self.max_video_length = max_video_length
        self.max_text_length = max_text_length
        self.lip_size = lip_size
        self.use_audio = use_audio
        self.use_visual = use_visual
        
        # í•œêµ­ì–´ ìì†Œ ì²˜ë¦¬ê¸°
        self.grapheme_processor = KoreanGraphemeProcessor()
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        print("ğŸ”¤ í•œêµ­ì–´ VALLR ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"ğŸ“ ë°ì´í„° ë£¨íŠ¸: {self.data_root}")
        print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {self.output_dir}")
        print(f"ğŸ“Š ì–´íœ˜ í¬ê¸°: {self.grapheme_processor.vocab_size}")
    
    def preprocess_dataset(self, split: str = 'train', batch_size: int = 1, max_retries: int = 100) -> Dict:
        """ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ (í•œ ê°œì”© ë°”ë¡œ ì €ì¥) - ìë™ ì¬ì‹œì‘ ê¸°ëŠ¥ í¬í•¨"""
        retry_count = 0
        
        while retry_count < max_retries:
            try:
                print(f"\nğŸ”„ {split} ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì‹œì‘... (ì‹œë„ {retry_count + 1}/{max_retries})")
                
                # ë°ì´í„°ì…‹ ìƒì„±
                dataset = KoreanLipReadingDataset(
                    data_root=str(self.data_root),
                    split=split,
                    max_video_length=self.max_video_length,
                    max_text_length=self.max_text_length,
                    lip_size=self.lip_size,
                    use_audio=self.use_audio,
                    use_visual=self.use_visual
                )
                
                total_samples = len(dataset)
                print(f"ğŸ“Š ì „ì²´ ìƒ˜í”Œ ìˆ˜: {total_samples}")
                print(f"ğŸ“¦ ì²˜ë¦¬ ë°©ì‹: í•œ ê°œì”© ë°”ë¡œ ì €ì¥")
                
                # ì´ë¯¸ ì²˜ë¦¬ëœ ìƒ˜í”Œ í™•ì¸
                processed_samples = set()
                samples_dir = self.output_dir / f"{split}_samples"
                if samples_dir.exists():
                    for sample_dir in samples_dir.iterdir():
                        if sample_dir.is_dir() and sample_dir.name.startswith("sample_"):
                            try:
                                sample_idx = int(sample_dir.name.split("_")[1])
                                # ì™„ì „íˆ ì²˜ë¦¬ëœ ìƒ˜í”Œì¸ì§€ í™•ì¸ (video_frames.npyì™€ text_indices.npyê°€ ëª¨ë‘ ì¡´ì¬)
                                video_file = sample_dir / "video_frames.npy"
                                text_file = sample_dir / "text_indices.npy"
                                if video_file.exists() and text_file.exists():
                                    processed_samples.add(sample_idx)
                            except ValueError:
                                continue
                
                if processed_samples:
                    print(f"ğŸ”„ ì´ë¯¸ ì²˜ë¦¬ëœ ìƒ˜í”Œ {len(processed_samples)}ê°œ ë°œê²¬")
                    print(f"ğŸ”„ {len(processed_samples)}ë²ˆì§¸ ìƒ˜í”Œë¶€í„° ì¬ì‹œì‘í•©ë‹ˆë‹¤")
                    start_idx = max(processed_samples) + 1
                else:
                    start_idx = 0
                    print(f"ğŸ†• ì²˜ìŒë¶€í„° ì‹œì‘í•©ë‹ˆë‹¤")
                
                # ì „ì²´ í†µê³„ ì •ë³´
                all_stats = {
                    'total_samples': total_samples,
                    'vocab_size': dataset.grapheme_processor.vocab_size,
                    'max_video_length': self.max_video_length,
                    'max_text_length': self.max_text_length,
                    'lip_size': self.lip_size,
                    'use_audio': self.use_audio,
                    'use_visual': self.use_visual,
                    'person_types': {},
                    'genders': {},
                    'environments': {},
                    'text_lengths': [],
                    'video_lengths': [],
                    'processed_samples': len(processed_samples),
                    'failed_samples': 0,
                    'resumed_from': start_idx,
                    'retry_count': retry_count
                }
                
                # í•œ ê°œì”© ì²˜ë¦¬í•˜ê³  ë°”ë¡œ ì €ì¥
                for i in range(start_idx, total_samples):
                    try:
                        print(f"\nğŸ“¹ ìƒ˜í”Œ {i+1}/{total_samples} ì²˜ë¦¬ ì¤‘...")
                        
                        # ì´ë¯¸ ì²˜ë¦¬ëœ ìƒ˜í”Œì¸ì§€ í™•ì¸
                        if i in processed_samples:
                            print(f"  â­ï¸ ìƒ˜í”Œ {i+1} ì´ë¯¸ ì²˜ë¦¬ë¨ - ê±´ë„ˆë›°ê¸°")
                            continue
                        
                        # ìƒ˜í”Œ ë¡œë“œ
                        sample = dataset[i]
                        
                        # í†µê³„ ì •ë³´ ìˆ˜ì§‘
                        person_type = sample['person_type']
                        gender = sample['gender']
                        environment = sample['environment']
                        
                        all_stats['person_types'][person_type] = all_stats['person_types'].get(person_type, 0) + 1
                        all_stats['genders'][gender] = all_stats['genders'].get(gender, 0) + 1
                        all_stats['environments'][environment] = all_stats['environments'].get(environment, 0) + 1
                        
                        all_stats['text_lengths'].append(sample['text_length'].item())
                        all_stats['video_lengths'].append(sample['video_length'].item())
                        
                        # ìƒ˜í”Œ ì •ë³´ ë°”ë¡œ ì €ì¥
                        self._save_single_sample(sample, i, split)
                        
                        all_stats['processed_samples'] += 1
                        
                        print(f"  âœ… ìƒ˜í”Œ {i+1} ì €ì¥ ì™„ë£Œ: ë¹„ë””ì˜¤ {sample['video_frames'].shape}, í…ìŠ¤íŠ¸ ê¸¸ì´ {sample['text_length'].item()}")
                        
                        # 10ê°œë§ˆë‹¤ ì¤‘ê°„ í†µê³„ ì¶œë ¥
                        if (i + 1) % 10 == 0:
                            print(f"  ğŸ“Š ì§„í–‰ë¥ : {i+1}/{total_samples} ({((i+1)/total_samples*100):.1f}%)")
                            print(f"  ğŸ“ˆ ì„±ê³µ: {all_stats['processed_samples']}, ì‹¤íŒ¨: {all_stats['failed_samples']}")
                        
                    except Exception as e:
                        print(f"  âŒ ìƒ˜í”Œ {i+1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        all_stats['failed_samples'] += 1
                        continue
                
                # ì „ì²´ í†µê³„ ê³„ì‚°
                if all_stats['text_lengths']:
                    all_stats['avg_text_length'] = np.mean(all_stats['text_lengths'])
                    all_stats['max_text_length_actual'] = max(all_stats['text_lengths'])
                if all_stats['video_lengths']:
                    all_stats['avg_video_length'] = np.mean(all_stats['video_lengths'])
                    all_stats['max_video_length_actual'] = max(all_stats['video_lengths'])
                
                # ì „ì²´ ê²°ê³¼ ì €ì¥
                self._save_processed_data(dataset, split, all_stats)
                
                print(f"\nâœ… {split} ë°ì´í„°ì…‹ ì „ì²˜ë¦¬ ì™„ë£Œ")
                print(f"ğŸ“Š ì„±ê³µ: {all_stats['processed_samples']}, ì‹¤íŒ¨: {all_stats['failed_samples']}")
                if start_idx > 0:
                    print(f"ğŸ”„ ì¬ì‹œì‘ ì§€ì : {start_idx}ë²ˆì§¸ ìƒ˜í”Œ")
                
                return all_stats
                
            except KeyboardInterrupt:
                print(f"\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨ (ì‹œë„ {retry_count + 1}/{max_retries})")
                retry_count += 1
                if retry_count < max_retries:
                    print(f"ğŸ”„ 5ì´ˆ í›„ ìë™ ì¬ì‹œì‘...")
                    import time
                    time.sleep(5)
                    continue
                else:
                    print(f"âŒ ìµœëŒ€ ì¬ì‹œì‘ íšŸìˆ˜({max_retries})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                    raise
                    
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ (ì‹œë„ {retry_count + 1}/{max_retries}): {e}")
                retry_count += 1
                if retry_count < max_retries:
                    print(f"ğŸ”„ 10ì´ˆ í›„ ìë™ ì¬ì‹œì‘...")
                    import time
                    time.sleep(10)
                    continue
                else:
                    print(f"âŒ ìµœëŒ€ ì¬ì‹œì‘ íšŸìˆ˜({max_retries})ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
                    raise
        
        print(f"âŒ ëª¨ë“  ì¬ì‹œì‘ ì‹œë„ ì‹¤íŒ¨")
        return {}
    
    def _save_single_sample(self, sample: Dict, sample_idx: int, split: str):
        """ë‹¨ì¼ ìƒ˜í”Œ ì •ë³´ ì €ì¥ (VALLR ëª¨ë¸ìš© numpy ë°°ì—´)"""
        # ìƒ˜í”Œë³„ ë””ë ‰í† ë¦¬ ìƒì„±
        sample_dir = self.output_dir / f"{split}_samples" / f"sample_{sample_idx:06d}"
        sample_dir.mkdir(parents=True, exist_ok=True)
        
        # ìƒ˜í”Œ ì •ë³´ ì €ì¥
        sample_info = {
            'sample_idx': sample_idx,
            'video_frames_shape': list(sample['video_frames'].shape),
            'text_indices_shape': list(sample['text_indices'].shape),
            'text_length': sample['text_length'].item(),
            'video_length': sample['video_length'].item(),
            'original_text': sample['original_text'],
            'sentence_id': sample['sentence_id'],
            'start_time': sample['start_time'],
            'end_time': sample['end_time'],
            'person_type': sample['person_type'],
            'gender': sample['gender'],
            'environment': sample['environment']
        }
        
        # ìƒ˜í”Œ ì •ë³´ íŒŒì¼ ì €ì¥
        info_file = sample_dir / "sample_info.json"
        with open(info_file, 'w', encoding='utf-8') as f:
            json.dump(sample_info, f, ensure_ascii=False, indent=2)
        
        # VALLR ëª¨ë¸ìš© ë°ì´í„° ì €ì¥
        import numpy as np
        
        # 1. ë¹„ë””ì˜¤ í”„ë ˆì„ ì €ì¥ (VALLR ì…ë ¥ìš©) - ë…¼ë¬¸ê³¼ ë™ì¼í•œ ìˆœì„œ
        video_frames = sample['video_frames'].numpy()  # [T, H, W, C]
        
        # VALLR ë…¼ë¬¸ í˜•ì‹: (T, C, H, W) ìˆœì„œë¡œ ë³€í™˜
        video_frames_tchw = np.transpose(video_frames, (0, 3, 1, 2))  # [T, C, H, W]
        
        video_file = sample_dir / "video_frames.npy"
        np.save(video_file, video_frames_tchw)
        
        # 2. í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤ ì €ì¥ (VALLR ë¼ë²¨ìš©)
        text_file = sample_dir / "text_indices.npy"
        np.save(text_file, sample['text_indices'].numpy())
        
        # 3. ê¸¸ì´ ì •ë³´ ì €ì¥
        length_file = sample_dir / "lengths.json"
        with open(length_file, 'w', encoding='utf-8') as f:
            json.dump({
                'text_length': sample['text_length'].item(),
                'video_length': sample['video_length'].item(),
                'num_frames': video_frames.shape[0],
                'tensor_shape': list(video_frames_tchw.shape),  # VALLR í˜•ì‹
                'original_shape': list(video_frames.shape)      # ì›ë³¸ í˜•ì‹
            }, f, ensure_ascii=False, indent=2)
        
        print(f"    ğŸ’¾ ë¹„ë””ì˜¤ í”„ë ˆì„ ì €ì¥: {video_frames_tchw.shape} (VALLR í˜•ì‹: TCHW)")
        print(f"    ğŸ’¾ í…ìŠ¤íŠ¸ ë¼ë²¨ ì €ì¥: {sample['text_indices'].shape}")
    
    def _process_batch(self, dataset: KoreanLipReadingDataset, start_idx: int, end_idx: int, batch_idx: int) -> Dict:
        """ë°°ì¹˜ ë‹¨ìœ„ ì²˜ë¦¬"""
        stats = {
            'batch_idx': batch_idx,
            'start_idx': start_idx,
            'end_idx': end_idx,
            'sample_count': end_idx - start_idx,
            'person_types': {},
            'genders': {},
            'environments': {},
            'text_lengths': [],
            'video_lengths': []
        }
        
        for i in range(start_idx, end_idx):
            try:
                sample = dataset[i]
                
                # ì¸êµ¬í†µê³„í•™ì  ì •ë³´
                person_type = sample['person_type']
                gender = sample['gender']
                environment = sample['environment']
                
                stats['person_types'][person_type] = stats['person_types'].get(person_type, 0) + 1
                stats['genders'][gender] = stats['genders'].get(gender, 0) + 1
                stats['environments'][environment] = stats['environments'].get(environment, 0) + 1
                
                # ê¸¸ì´ ì •ë³´
                stats['text_lengths'].append(sample['text_length'].item())
                stats['video_lengths'].append(sample['video_length'].item())
                
                if (i - start_idx + 1) % 100 == 0:
                    print(f"    ğŸ“Š {i - start_idx + 1}/{end_idx - start_idx} ìƒ˜í”Œ ì²˜ë¦¬ ì™„ë£Œ")
                
            except Exception as e:
                print(f"    âš ï¸ ìƒ˜í”Œ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        return stats
    
    def _save_batch_data(self, dataset: KoreanLipReadingDataset, start_idx: int, end_idx: int, batch_idx: int, split: str):
        """ë°°ì¹˜ ë°ì´í„° ì €ì¥"""
        batch_dir = self.output_dir / f"{split}_batch_{batch_idx:03d}"
        batch_dir.mkdir(exist_ok=True)
        
        # ë°°ì¹˜ ì •ë³´ ì €ì¥
        batch_info = {
            'batch_idx': batch_idx,
            'start_idx': start_idx,
            'end_idx': end_idx,
            'sample_count': end_idx - start_idx,
            'samples': []
        }
        
        # ìƒ˜í”Œ ì •ë³´ ìˆ˜ì§‘ (ì²˜ìŒ 10ê°œë§Œ)
        for i in range(start_idx, min(start_idx + 10, end_idx)):
            try:
                sample = dataset[i]
                sample_info = {
                    'index': i,
                    'video_frames_shape': list(sample['video_frames'].shape),
                    'text_indices_shape': list(sample['text_indices'].shape),
                    'text_length': sample['text_length'].item(),
                    'video_length': sample['video_length'].item(),
                    'original_text': sample['original_text'],
                    'sentence_id': sample['sentence_id'],
                    'start_time': sample['start_time'],
                    'end_time': sample['end_time'],
                    'person_type': sample['person_type'],
                    'gender': sample['gender'],
                    'environment': sample['environment']
                }
                batch_info['samples'].append(sample_info)
            except Exception as e:
                print(f"    âš ï¸ ìƒ˜í”Œ {i} ì •ë³´ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # ë°°ì¹˜ ì •ë³´ ì €ì¥
        batch_file = batch_dir / "batch_info.json"
        with open(batch_file, 'w', encoding='utf-8') as f:
            json.dump(batch_info, f, ensure_ascii=False, indent=2)
        
        print(f"    ğŸ’¾ ë°°ì¹˜ {batch_idx} ì •ë³´ ì €ì¥ ì™„ë£Œ: {batch_file}")
    
    def _collect_statistics(self, dataset: KoreanLipReadingDataset) -> Dict:
        """ë°ì´í„°ì…‹ í†µê³„ ì •ë³´ ìˆ˜ì§‘"""
        print("ğŸ“Š í†µê³„ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        
        stats = {
            'total_samples': len(dataset),
            'vocab_size': dataset.grapheme_processor.vocab_size,
            'max_video_length': self.max_video_length,
            'max_text_length': self.max_text_length,
            'lip_size': self.lip_size,
            'use_audio': self.use_audio,
            'use_visual': self.use_visual,
            'person_types': {},
            'genders': {},
            'environments': {},
            'text_lengths': [],
            'video_lengths': []
        }
        
        # ìƒ˜í”Œë³„ í†µê³„ ìˆ˜ì§‘
        for i in range(min(100, len(dataset))):  # ì²˜ìŒ 100ê°œ ìƒ˜í”Œë§Œ ë¶„ì„
            try:
                sample = dataset[i]
                
                # ì¸êµ¬í†µê³„í•™ì  ì •ë³´
                person_type = sample['person_type']
                gender = sample['gender']
                environment = sample['environment']
                
                stats['person_types'][person_type] = stats['person_types'].get(person_type, 0) + 1
                stats['genders'][gender] = stats['genders'].get(gender, 0) + 1
                stats['environments'][environment] = stats['environments'].get(environment, 0) + 1
                
                # ê¸¸ì´ ì •ë³´
                stats['text_lengths'].append(sample['text_length'].item())
                stats['video_lengths'].append(sample['video_length'].item())
                
            except Exception as e:
                print(f"âš ï¸ ìƒ˜í”Œ {i} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # í‰ê·  ê¸¸ì´ ê³„ì‚°
        if stats['text_lengths']:
            stats['avg_text_length'] = np.mean(stats['text_lengths'])
            stats['max_text_length_actual'] = max(stats['text_lengths'])
        if stats['video_lengths']:
            stats['avg_video_length'] = np.mean(stats['video_lengths'])
            stats['max_video_length_actual'] = max(stats['video_lengths'])
        
        return stats
    
    def _save_processed_data(self, dataset: KoreanLipReadingDataset, split: str, stats: Dict):
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        print("ğŸ’¾ ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì¤‘...")
        
        # í†µê³„ ì •ë³´ ì €ì¥
        stats_file = self.output_dir / f"{split}_stats.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“Š í†µê³„ ì •ë³´ ì €ì¥ ì™„ë£Œ: {stats_file}")
        
        # ì–´íœ˜ ì •ë³´ ì €ì¥
        vocab_info = dataset.grapheme_processor.get_vocab_info()
        vocab_file = self.output_dir / f"{split}_vocab.json"
        with open(vocab_file, 'w', encoding='utf-8') as f:
            json.dump(vocab_info, f, ensure_ascii=False, indent=2)
        print(f"ğŸ”¤ ì–´íœ˜ ì •ë³´ ì €ì¥ ì™„ë£Œ: {vocab_file}")
        
        # ë°ì´í„° ë§¤í•‘ ì •ë³´ ì €ì¥
        mapping_file = self.output_dir / f"{split}_mapping.json"
        with open(mapping_file, 'w', encoding='utf-8') as f:
            json.dump(dataset.data_mapping, f, ensure_ascii=False, indent=2)
        print(f"ğŸ—‚ï¸ ë°ì´í„° ë§¤í•‘ ì •ë³´ ì €ì¥ ì™„ë£Œ: {mapping_file}")
        
        # ì‹¤ì œ ì „ì²˜ë¦¬ëœ ë°ì´í„° ìƒ˜í”Œ ì €ì¥ (ì²˜ìŒ 10ê°œ)
        print("ğŸ¬ ì‹¤ì œ ë°ì´í„° ìƒ˜í”Œ ì €ì¥ ì¤‘...")
        sample_data = []
        for i in range(min(10, len(dataset))):
            try:
                sample = dataset[i]
                sample_info = {
                    'index': i,
                    'video_frames_shape': list(sample['video_frames'].shape),
                    'text_indices_shape': list(sample['text_indices'].shape),
                    'text_length': sample['text_length'].item(),
                    'video_length': sample['video_length'].item(),
                    'original_text': sample['original_text'],
                    'sentence_id': sample['sentence_id'],
                    'start_time': sample['start_time'],
                    'end_time': sample['end_time'],
                    'person_type': sample['person_type'],
                    'gender': sample['gender'],
                    'environment': sample['environment']
                }
                sample_data.append(sample_info)
                print(f"  ğŸ“¹ ìƒ˜í”Œ {i}: ë¹„ë””ì˜¤ {sample['video_frames'].shape}, í…ìŠ¤íŠ¸ ê¸¸ì´ {sample['text_length'].item()}")
            except Exception as e:
                print(f"  âš ï¸ ìƒ˜í”Œ {i} ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # ìƒ˜í”Œ ì •ë³´ ì €ì¥
        sample_file = self.output_dir / f"{split}_samples.json"
        with open(sample_file, 'w', encoding='utf-8') as f:
            json.dump(sample_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“‹ ìƒ˜í”Œ ì •ë³´ ì €ì¥ ì™„ë£Œ: {sample_file}")
        
        print(f"âœ… ì „ì²˜ë¦¬ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {self.output_dir}")
        print(f"ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
        for file in self.output_dir.glob(f"{split}_*"):
            print(f"  - {file.name}")
    
    def create_dataloader(self, split: str = 'train', batch_size: int = 8, num_workers: int = 4) -> DataLoader:
        """ë°ì´í„°ë¡œë” ìƒì„±"""
        return create_korean_dataloader(
            data_root=str(self.data_root),
            batch_size=batch_size,
            split=split,
            num_workers=num_workers,
            max_video_length=self.max_video_length,
            max_text_length=self.max_text_length,
            lip_size=self.lip_size,
            use_audio=self.use_audio,
            use_visual=self.use_visual
        )
    
    def test_preprocessing(self):
        """ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ§ª ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        try:
            # ë°ì´í„°ì…‹ ì§ì ‘ ìƒì„± (ë°°ì¹˜ ì²˜ë¦¬ ì—†ì´)
            dataset = KoreanLipReadingDataset(
                data_root=str(self.data_root),
                split='train',
                max_video_length=self.max_video_length,
                max_text_length=self.max_text_length,
                lip_size=self.lip_size,
                use_audio=self.use_audio,
                use_visual=self.use_visual
            )
            
            print(f"ğŸ“Š ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)}")
            
            # ì²« ë²ˆì§¸ ìƒ˜í”Œ ë¡œë“œ
            sample = dataset[0]
            print("âœ… ìƒ˜í”Œ ë¡œë“œ ì„±ê³µ!")
            print(f"ğŸ“¹ ë¹„ë””ì˜¤ í”„ë ˆì„: {sample['video_frames'].shape}")
            print(f"ğŸ“ í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤: {sample['text_indices'].shape}")
            print(f"ğŸ“ í…ìŠ¤íŠ¸ ê¸¸ì´: {sample['text_length']}")
            print(f"ğŸ¬ ë¹„ë””ì˜¤ ê¸¸ì´: {sample['video_length']}")
            print(f"ğŸ‘¤ ì›ë³¸ í…ìŠ¤íŠ¸: {sample['original_text']}")
            print(f"ğŸ†” ë¬¸ì¥ ID: {sample['sentence_id']}")
            print(f"â° ì‹œì‘ ì‹œê°„: {sample['start_time']}")
            print(f"â° ì¢…ë£Œ ì‹œê°„: {sample['end_time']}")
            
            if self.use_audio and 'mel_spectrogram' in sample:
                print(f"ğŸµ ë©œìŠ¤í™íŠ¸ë¡œê·¸ë¨: {sample['mel_spectrogram'].shape}")
            
            # ë‘ ë²ˆì§¸ ìƒ˜í”Œë„ í™•ì¸ (ë‹¤ë¥¸ ê¸¸ì´)
            sample2 = dataset[1]
            print(f"\nğŸ“¹ ë‘ ë²ˆì§¸ ìƒ˜í”Œ ë¹„ë””ì˜¤ í”„ë ˆì„: {sample2['video_frames'].shape}")
            print(f"ğŸ“ ë‘ ë²ˆì§¸ ìƒ˜í”Œ í…ìŠ¤íŠ¸ ì¸ë±ìŠ¤: {sample2['text_indices'].shape}")
            print(f"ğŸ‘¤ ë‘ ë²ˆì§¸ ìƒ˜í”Œ ì›ë³¸ í…ìŠ¤íŠ¸: {sample2['original_text']}")
                
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False
        
        return True

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    import sys
    
    # ëª…ë ¹í–‰ ì¸ìˆ˜ ì²˜ë¦¬
    if len(sys.argv) > 1:
        # ëª…ë ¹í–‰ ì¸ìˆ˜ê°€ ìˆìœ¼ë©´ argparse ì‚¬ìš©
        parser = argparse.ArgumentParser(description="í•œêµ­ì–´ VALLR ì „ì²˜ë¦¬")
        parser.add_argument("--data_root", type=str, default="009.ë¦½ë¦¬ë”©(ì…ëª¨ì–‘) ìŒì„±ì¸ì‹ ë°ì´í„°",
                           help="ë°ì´í„° ë£¨íŠ¸ ê²½ë¡œ")
        parser.add_argument("--output_dir", type=str, default="processed_data",
                           help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
        parser.add_argument("--max_video_length", type=int, default=768,
                           help="ìµœëŒ€ ë¹„ë””ì˜¤ ê¸¸ì´ (VALLR ë…¼ë¬¸: ViT ìµœëŒ€ ì…ë ¥ ê¸¸ì´)")
        parser.add_argument("--max_text_length", type=int, default=300,
                           help="ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´")
        parser.add_argument("--lip_size", type=int, nargs=2, default=[224, 224],
                           help="ë¦½ ì´ë¯¸ì§€ í¬ê¸° (VALLR ë…¼ë¬¸: 224x224)")
        parser.add_argument("--use_audio", action="store_true", default=False,
                           help="ì˜¤ë””ì˜¤ ì‚¬ìš© (VALLRì€ ë¹„ë””ì˜¤ë§Œ ì‚¬ìš©)")
        parser.add_argument("--use_visual", action="store_true", default=True,
                           help="ë¹„ì£¼ì–¼ ì‚¬ìš©")
        parser.add_argument("--test_only", action="store_true",
                           help="í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰")
        parser.add_argument("--batch_size", type=int, default=1,
                           help="ë°°ì¹˜ í¬ê¸° (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°ì ˆìš©, 1=í•œê°œì”© ì²˜ë¦¬)")
        
        args = parser.parse_args()
    else:
        # ê¸°ë³¸ê°’ìœ¼ë¡œ ì‹¤í–‰
        class Args:
            data_root = "009.ë¦½ë¦¬ë”©(ì…ëª¨ì–‘) ìŒì„±ì¸ì‹ ë°ì´í„°"
            output_dir = "processed_data"
            max_video_length = 768
            max_text_length = 300
            lip_size = [224, 224]
            use_audio = False
            use_visual = True
            test_only = False
            batch_size = 1
        
        args = Args()
    
    try:
        print("ğŸš€ í•œêµ­ì–´ VALLR ì „ì²˜ë¦¬ ì‹œì‘...")
        print(f"ğŸ“ ë°ì´í„° ë£¨íŠ¸: {args.data_root}")
        print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output_dir}")
        
        # ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        preprocessor = KoreanVALLRPreprocessor(
            data_root=args.data_root,
            output_dir=args.output_dir,
            max_video_length=args.max_video_length,
            max_text_length=args.max_text_length,
            lip_size=tuple(args.lip_size),
            use_audio=args.use_audio,
            use_visual=args.use_visual
        )
        
        if args.test_only:
            # í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰
            print("\nğŸ§ª ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
            success = preprocessor.test_preprocessing()
            if success:
                print("ğŸ‰ ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
            else:
                print("âŒ ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")
                sys.exit(1)
        else:
            # ì „ì²´ ì „ì²˜ë¦¬ ì‹¤í–‰
            print("\nğŸ”„ í›ˆë ¨ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
            train_stats = preprocessor.preprocess_dataset('train', args.batch_size)
            print(f"âœ… í›ˆë ¨ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {train_stats['total_samples']}ê°œ ìƒ˜í”Œ")
            
            # ê²€ì¦ ë°ì´í„° ì „ì²˜ë¦¬ (ì„ íƒì )
            try:
                print("\nğŸ”„ ê²€ì¦ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘...")
                val_stats = preprocessor.preprocess_dataset('val', args.batch_size)
                print(f"âœ… ê²€ì¦ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {val_stats['total_samples']}ê°œ ìƒ˜í”Œ")
            except Exception as e:
                print(f"âš ï¸ ê²€ì¦ ë°ì´í„° ì „ì²˜ë¦¬ ê±´ë„ˆëœ€: {e}")
            
            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            print("\nğŸ§ª ìµœì¢… í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
            preprocessor.test_preprocessing()
            
            print("\nğŸ‰ ì „ì²˜ë¦¬ ì™„ë£Œ!")
            print(f"ğŸ“ ê²°ê³¼ íŒŒì¼ë“¤:")
            for file in preprocessor.output_dir.glob("*"):
                print(f"  - {file.name}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()