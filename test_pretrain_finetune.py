# ì‚¬ì „í•™ìŠµ + ë³¸í•™ìŠµ ìˆœì°¨ í…ŒìŠ¤íŠ¸
# ë¹ ë¥¸ ì‹¤í—˜ì„ ìœ„í•´ ì‘ì€ ë°ì´í„°ì…‹ ì‚¬ìš©

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
import cv2
import mediapipe as mp
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import json
import pickle
import time
import random
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')
import argparse
import os
import shutil
from datetime import datetime
import pandas as pd
import seaborn as sns

# train.pyì—ì„œ í•„ìš”í•œ í´ë˜ìŠ¤ë“¤ import
from train import (
    Config, LipROIExtractor, KoreanGraphemeProcessor,
    UnlabeledVideoDataset, LabeledVideoDataset, CrawlerVideoDataset,
    VisualFrontend, PositionalEncoding, TemporalEncoder, 
    MaskedVideoModel, LipReadingModel, PretrainTrainer, 
    FinetuneTrainer, ModelEvaluator, collate_fn,
    setup_directories
)

def collate_fn_pretrain(batch):
    """ì‚¬ì „í•™ìŠµìš© collate í•¨ìˆ˜"""
    frames = torch.stack([item['frames'] for item in batch])
    return frames

def test_pretrain_finetune():
    """ì‚¬ì „í•™ìŠµ + ë³¸í•™ìŠµ ìˆœì°¨ í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ ì‚¬ì „í•™ìŠµ + ë³¸í•™ìŠµ ìˆœì°¨ í…ŒìŠ¤íŠ¸ ì‹œì‘!")
    
    # ì„¤ì •
    config = Config()
    # ë¹ ë¥¸ ì‹¤í—˜ì„ ìœ„í•´ ì—í¬í¬ ìˆ˜ ì¡°ì •
    config.PRETRAIN_EPOCHS = 5   # 100 -> 5
    config.FINETUNE_EPOCHS = 3   # 50 -> 3
    config.BATCH_SIZE = 2         # ë©”ëª¨ë¦¬ ì ˆì•½
    
    device = config.DEVICE
    print(f"âš™ï¸ ì„¤ì •:")
    print(f"   - ì‚¬ì „í•™ìŠµ ì—í¬í¬: {config.PRETRAIN_EPOCHS}")
    print(f"   - ë³¸í•™ìŠµ ì—í¬í¬: {config.FINETUNE_EPOCHS}")
    print(f"   - ë°°ì¹˜ í¬ê¸°: {config.BATCH_SIZE}")
    print(f"   - ë””ë°”ì´ìŠ¤: {device}")
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬
    import gc
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # 1ë‹¨ê³„: ì‚¬ì „í•™ìŠµ ë°ì´í„° ì¤€ë¹„
    print("\nğŸ“Š 1ë‹¨ê³„: ì‚¬ì „í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
    
    # data/lip_videosì—ì„œ ë¼ë²¨ë§ë˜ì§€ ì•Šì€ ë¹„ë””ì˜¤ íŒŒì¼ë“¤ ì‚¬ìš© (ì‚¬ì „í•™ìŠµìš©)
    lip_videos_dir = Path("data/lip_videos")
    
    # ì‚¬ì „í•™ìŠµìš© ë¹„ë””ì˜¤ íŒŒì¼ë“¤ ì°¾ê¸°
    video_files = list(lip_videos_dir.glob("*.mp4"))
    if not video_files:
        raise FileNotFoundError(f"ì‚¬ì „í•™ìŠµìš© ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {lip_videos_dir}")
    
    # ëª¨ë“  ë¹„ë””ì˜¤ íŒŒì¼ ì‚¬ìš©
    pretrain_videos = video_files
    print(f"ğŸ“Š ì‚¬ì „í•™ìŠµìš© ë¹„ë””ì˜¤: {len(pretrain_videos)}ê°œ")
    for video in pretrain_videos:
        print(f"   - {video.name}")
    
    # ì‚¬ì „í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„±
    try:
        pretrain_dataset = PretrainUnlabeledDataset(pretrain_videos)
        print(f"âœ… ì‚¬ì „í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {len(pretrain_dataset)}ê°œ ìƒ˜í”Œ")
    except Exception as e:
        print(f"âš ï¸ ì‚¬ì „í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {e}")
        raise
    
    # ì‚¬ì „í•™ìŠµ ë°ì´í„° ë¡œë”
    pretrain_loader = DataLoader(
        pretrain_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=True,
        num_workers=0,
        drop_last=True,
        collate_fn=collate_fn_pretrain
    )
    
    # 2ë‹¨ê³„: ì‚¬ì „í•™ìŠµ
    print("\nğŸ¯ 2ë‹¨ê³„: ì‚¬ì „í•™ìŠµ ì‹œì‘...")
    
    # ì‚¬ì „í•™ìŠµ ëª¨ë¸ ìƒì„±
    pretrain_model = MaskedVideoModel().to(device)
    
    # ëª¨ë¸ ì •ë³´ ì¶œë ¥
    total_params = sum(p.numel() for p in pretrain_model.parameters())
    print(f"ğŸ—ï¸ ì‚¬ì „í•™ìŠµ ëª¨ë¸ íŒŒë¼ë¯¸í„°: {total_params:,}ê°œ")
    
    # ì‚¬ì „í•™ìŠµ ì‹¤í–‰
    pretrain_trainer = PretrainTrainer(pretrain_model, pretrain_loader, device)
    pretrain_trainer.train(epochs=config.PRETRAIN_EPOCHS)
    
    # ì‚¬ì „í•™ìŠµëœ ê°€ì¤‘ì¹˜ ì €ì¥
    pretrain_save_path = Path("test_results") / "pretrained_weights.pt"
    pretrain_save_path.parent.mkdir(exist_ok=True)
    
    # ì¸ì½”ë” ë¶€ë¶„ë§Œ ì €ì¥
    encoder_weights = {
        'visual_frontend': pretrain_model.visual_frontend.state_dict(),
        'temporal_encoder': pretrain_model.temporal_encoder.state_dict()
    }
    torch.save(encoder_weights, pretrain_save_path)
    print(f"ğŸ’¾ ì‚¬ì „í•™ìŠµ ì¸ì½”ë” ê°€ì¤‘ì¹˜ ì €ì¥: {pretrain_save_path}")
    
    # 3ë‹¨ê³„: ë³¸í•™ìŠµ ë°ì´í„° ì¤€ë¹„
    print("\nğŸ“Š 3ë‹¨ê³„: ë³¸í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
    
    # TL48ì—ì„œ ë¼ë²¨ë§ëœ ë°ì´í„° ì‚¬ìš©
    labeled_data_dir = Path("009.ë¦½ë¦¬ë”©(ì…ëª¨ì–‘) ìŒì„±ì¸ì‹ ë°ì´í„°/01.ë°ì´í„°/1.Training/ìì†Œ_ë¼ë²¨ë§ë°ì´í„°")
    tl48_label_dir = labeled_data_dir / "TL48" / "ì†ŒìŒí™˜ê²½1" / "C(ì¼ë°˜ì¸)" / "M(ë‚¨ì„±)" / "M(ë‚¨ì„±)_24"
    
    # ì²« ë²ˆì§¸ JSON íŒŒì¼ ì‚¬ìš©
    json_files = list(tl48_label_dir.glob("*.json"))
    if not json_files:
        raise FileNotFoundError(f"ë¼ë²¨ë§ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {tl48_label_dir}")
    
    json_path = json_files[0]
    # JSON íŒŒì¼ëª…ì—ì„œ _jasoë¥¼ ì œê±°í•˜ê³  .mp4 í™•ì¥ì ì¶”ê°€
    video_filename = json_path.stem.replace('_jaso', '') + ".mp4"
    # ì›ì²œë°ì´í„° ê²½ë¡œì—ì„œ í•´ë‹¹ ë¹„ë””ì˜¤ íŒŒì¼ ì°¾ê¸°
    source_data_dir = Path("009.ë¦½ë¦¬ë”©(ì…ëª¨ì–‘) ìŒì„±ì¸ì‹ ë°ì´í„°/01.ë°ì´í„°/1.Training/ì›ì²œë°ì´í„°")
    ts48_source_dir = source_data_dir / "TS48" / "ì†ŒìŒí™˜ê²½1" / "C(ì¼ë°˜ì¸)" / "M(ë‚¨ì„±)" / "M(ë‚¨ì„±)_24"
    video_path = ts48_source_dir / video_filename
    
    if not video_path.exists():
        raise FileNotFoundError(f"ë¹„ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {video_path}")
    
    print(f"ğŸ“Š ë³¸í•™ìŠµ ë°ì´í„°:")
    print(f"   - ë¹„ë””ì˜¤: {video_path.name}")
    print(f"   - ë¼ë²¨: {json_path.name}")
    
    # ë³¸í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„±
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if isinstance(data, list):
            annotations = data[0]['Sentence_info']
        else:
            annotations = data.get('Sentence_info', [])
        
        print(f"ğŸ“Š JSON êµ¬ì¡°: {type(data)}, ì–´ë…¸í…Œì´ì…˜ ìˆ˜: {len(annotations)}")
        
        finetune_dataset = CustomLabeledDataset(str(video_path), annotations)
        print(f"âœ… ë³¸í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ: {len(finetune_dataset)}ê°œ ìƒ˜í”Œ")
    except Exception as e:
        print(f"âš ï¸ ë³¸í•™ìŠµ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {e}")
        raise
    
    # ë°ì´í„° ë¶„í•  (8:2)
    total_samples = len(finetune_dataset)
    train_size = int(total_samples * 0.8)
    val_size = total_samples - train_size
    
    indices = list(range(total_samples))
    random.shuffle(indices)
    
    train_indices = indices[:train_size]
    val_indices = indices[train_size:]
    
    train_dataset = torch.utils.data.Subset(finetune_dataset, train_indices)
    val_dataset = torch.utils.data.Subset(finetune_dataset, val_indices)
    
    print(f"ğŸ“ˆ í›ˆë ¨ ë°ì´í„°: {len(train_dataset)}ê°œ ìƒ˜í”Œ")
    print(f"ğŸ“Š ê²€ì¦ ë°ì´í„°: {len(val_dataset)}ê°œ ìƒ˜í”Œ")
    
    # ë³¸í•™ìŠµ ë°ì´í„° ë¡œë”
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=True,
        num_workers=0,
        collate_fn=collate_fn,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=False,
        num_workers=0,
        collate_fn=collate_fn,
        drop_last=True
    )
    
    # 4ë‹¨ê³„: ë³¸í•™ìŠµ
    print("\nğŸ¯ 4ë‹¨ê³„: ë³¸í•™ìŠµ ì‹œì‘...")
    
    # ë³¸í•™ìŠµ ëª¨ë¸ ìƒì„±
    finetune_model = LipReadingModel(vocab_size=config.VOCAB_SIZE).to(device)
    
    # ì‚¬ì „í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
    try:
        # ì‚¬ì „í•™ìŠµ ëª¨ë¸ì˜ ì¸ì½”ë” ë¶€ë¶„ë§Œ ë¡œë“œ
        pretrained_state = torch.load(pretrain_save_path, map_location='cpu')
        
        # Visual Frontend ê°€ì¤‘ì¹˜ ë¡œë“œ
        if 'visual_frontend' in pretrained_state:
            finetune_model.visual_frontend.load_state_dict(pretrained_state['visual_frontend'])
            print("âœ… Visual frontend ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
        
        # Temporal Encoder ê°€ì¤‘ì¹˜ ë¡œë“œ
        if 'temporal_encoder' in pretrained_state:
            finetune_model.temporal_encoder.load_state_dict(pretrained_state['temporal_encoder'])
            print("âœ… Temporal encoder ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
            
    except Exception as e:
        print(f"âš ï¸ ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("ì²˜ìŒë¶€í„° í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # ë³¸í•™ìŠµ ì‹¤í–‰
    finetune_trainer = FinetuneTrainer(finetune_model, train_loader, val_loader, device)
    finetune_trainer.train(epochs=config.FINETUNE_EPOCHS)
    
    # 5ë‹¨ê³„: í‰ê°€
    print("\nğŸ” 5ë‹¨ê³„: ëª¨ë¸ í‰ê°€ ì¤‘...")
    evaluator = ModelEvaluator(finetune_model, device)
    metrics = evaluator.evaluate(val_loader)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "="*50)
    print("ğŸ“Š ì‚¬ì „í•™ìŠµ + ë³¸í•™ìŠµ ê²°ê³¼")
    print("="*50)
    
    print(f"ğŸ“ˆ í›ˆë ¨ ì™„ë£Œ!")
    print(f"ğŸ“Š í‰ê°€ ê²°ê³¼:")
    print(f"   - GER (ìì†Œ ì˜¤ë¥˜ìœ¨): {metrics['ger']:.4f} ({metrics['ger']*100:.2f}%)")
    print(f"   - CER (ìŒì ˆ ì˜¤ë¥˜ìœ¨): {metrics['cer']:.4f} ({metrics['cer']*100:.2f}%)")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_save_path = Path("test_results") / "pretrain_finetune_model.pt"
    torch.save(finetune_model.state_dict(), final_save_path)
    print(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥: {final_save_path}")
    
    return metrics

class PretrainUnlabeledDataset(Dataset):
    """ì‚¬ì „í•™ìŠµìš© ë¼ë²¨ë§ë˜ì§€ ì•Šì€ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ - ì „ì²´ ì˜ìƒ í™œìš©"""
    
    def __init__(self, video_paths):
        self.video_paths = video_paths
        self.lip_extractor = LipROIExtractor()
        self.target_length = 64  # ê³ ì •ëœ ì‹œí€€ìŠ¤ ê¸¸ì´
        self.clip_duration = 2.0  # 2ì´ˆ í´ë¦½
        
        # ê° ì˜ìƒì˜ ì´ ê¸¸ì´ì™€ í´ë¦½ ìˆ˜ ê³„ì‚°
        self.video_info = []
        total_clips = 0
        
        for video_path in video_paths:
            cap = cv2.VideoCapture(str(video_path))
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = frame_count / fps
            cap.release()
            
            # ì „ì²´ ì˜ìƒì„ 2ì´ˆ í´ë¦½ìœ¼ë¡œ ë‚˜ëˆ„ê¸° (ì¤‘ë³µ ì—†ì´)
            num_clips = int(duration / self.clip_duration)
            if num_clips < 1:
                num_clips = 1  # ìµœì†Œ 1ê°œ í´ë¦½ ë³´ì¥
            
            self.video_info.append({
                'path': video_path,
                'duration': duration,
                'fps': fps,
                'frame_count': frame_count,
                'num_clips': num_clips,
                'start_clip_idx': total_clips
            })
            
            total_clips += num_clips
        
        self.total_clips = total_clips
        
        print(f"ğŸ“Š ì‚¬ì „í•™ìŠµ ë¹„ë””ì˜¤: {len(self.video_paths)}ê°œ")
        for info in self.video_info:
            print(f"   - {info['path'].name}: {info['duration']:.1f}ì´ˆ â†’ {info['num_clips']}ê°œ í´ë¦½")
        print(f"ğŸ“Š ì´ í´ë¦½ ìˆ˜: {total_clips}ê°œ")
    
    def __len__(self):
        return self.total_clips
    
    def __getitem__(self, idx):
        # ì–´ë–¤ ì˜ìƒì˜ ëª‡ ë²ˆì§¸ í´ë¦½ì¸ì§€ ê³„ì‚°
        video_idx = 0
        local_clip_idx = idx
        
        for i, info in enumerate(self.video_info):
            if local_clip_idx < info['num_clips']:
                video_idx = i
                break
            local_clip_idx -= info['num_clips']
        
        video_info = self.video_info[video_idx]
        video_path = video_info['path']
        duration = video_info['duration']
        
        # ì—°ì†ì ì¸ í´ë¦½ ìƒì„± (ì „ì²´ ì˜ìƒì„ ìˆœì°¨ì ìœ¼ë¡œ ì‚¬ìš©)
        start_time = local_clip_idx * self.clip_duration
        end_time = min(start_time + self.clip_duration, duration)
        
        # ì…ìˆ  ì‹œí€€ìŠ¤ ì¶”ì¶œ
        lip_sequence = self.lip_extractor.extract_video_lip_sequence(
            video_path, start_time, end_time
        )
        
        # í”„ë ˆì„ ê¸¸ì´ ë§ì¶”ê¸° (íŒ¨ë”© ë˜ëŠ” ìë¥´ê¸°)
        if len(lip_sequence) < self.target_length:
            # íŒ¨ë”©: ë§ˆì§€ë§‰ í”„ë ˆì„ìœ¼ë¡œ ì±„ìš°ê¸°
            padding_frames = [lip_sequence[-1]] * (self.target_length - len(lip_sequence))
            lip_sequence = np.concatenate([lip_sequence, padding_frames], axis=0)
        elif len(lip_sequence) > self.target_length:
            # ìë¥´ê¸°: ì•ë¶€ë¶„ ì‚¬ìš©
            lip_sequence = lip_sequence[:self.target_length]
        
        return {
            'frames': torch.FloatTensor(lip_sequence),
            'frame_length': self.target_length
        }

class CustomLabeledDataset(Dataset):
    """ì»¤ìŠ¤í…€ ë¼ë²¨ë§ëœ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹ (ìì†Œ ë¼ë²¨ë§ ë°ì´í„°ìš©)"""
    
    def __init__(self, video_path, annotations):
        self.video_path = Path(video_path)
        self.annotations = annotations
        self.lip_extractor = LipROIExtractor()
        self.grapheme_processor = KoreanGraphemeProcessor()
        
        print(f"ğŸ“Š ë¼ë²¨ ë°ì´í„°: {len(self.annotations)}ê°œ ë¬¸ì¥")
    
    def __len__(self):
        return len(self.annotations)
    
    def __getitem__(self, idx):
        annotation = self.annotations[idx]
        
        # ì‹œê°„ êµ¬ê°„ ì •ë³´
        start_time = annotation['start_time']
        end_time = annotation['end_time']
        
        # ìì†Œ ë¼ë²¨ (ì´ë¯¸ ë¶„í•´ëœ ìƒíƒœ)
        sentence_graphemes = annotation['sentence_text']  # ìì†Œ ë¦¬ìŠ¤íŠ¸
        grapheme_indices = []
        
        for grapheme in sentence_graphemes:
            if grapheme in self.grapheme_processor.grapheme_to_idx:
                grapheme_indices.append(self.grapheme_processor.grapheme_to_idx[grapheme])
        
        # ì…ìˆ  ì‹œí€€ìŠ¤ ì¶”ì¶œ
        lip_sequence = self.lip_extractor.extract_video_lip_sequence(
            self.video_path, start_time, end_time
        )
        
        return {
            'frames': torch.FloatTensor(lip_sequence),
            'graphemes': torch.LongTensor(grapheme_indices),
            'frame_length': len(lip_sequence),
            'grapheme_length': len(grapheme_indices),
            'text': ''.join(sentence_graphemes)
        }

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ì‚¬ì „í•™ìŠµ + ë³¸í•™ìŠµ ìˆœì°¨ í…ŒìŠ¤íŠ¸')
    parser.add_argument('--run', action='store_true', help='í…ŒìŠ¤íŠ¸ ì‹¤í–‰')
    
    args = parser.parse_args()
    
    if args.run:
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        metrics = test_pretrain_finetune()
        print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    else:
        print("ğŸš€ ì‚¬ì „í•™ìŠµ + ë³¸í•™ìŠµ ìˆœì°¨ í…ŒìŠ¤íŠ¸")
        print("\nì‚¬ìš©ë²•:")
        print("python test_pretrain_finetune.py --run")
        print("\nì´ í…ŒìŠ¤íŠ¸ëŠ” ë‹¤ìŒì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:")
        print("1. ì‚¬ì „í•™ìŠµ: ë¼ë²¨ë§ë˜ì§€ ì•Šì€ ë¹„ë””ì˜¤ë¡œ ë§ˆìŠ¤í‚¹ ë¹„ë””ì˜¤ ëª¨ë¸ë§")
        print("2. ë³¸í•™ìŠµ: ë¼ë²¨ë§ëœ ë°ì´í„°ë¡œ ìì†Œ ì˜ˆì¸¡ í›ˆë ¨")
        print("3. ì‚¬ì „í•™ìŠµëœ ê°€ì¤‘ì¹˜ë¥¼ ë³¸í•™ìŠµì— ì „ì´")
        print("4. ì„±ëŠ¥ ì§€í‘œ ì¶œë ¥")

if __name__ == "__main__":
    main() 