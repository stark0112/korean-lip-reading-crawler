#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
λ³Έν•™μµλ§ μ§„ν–‰ν•λ” ν…μ¤νΈ μ¤ν¬λ¦½νΈ
κΈ°μ΅΄ μ‚¬μ „ν•™μµλ κ°€μ¤‘μΉλ¥Ό μ‚¬μ©ν•μ—¬ λ³Έν•™μµ μν–‰
"""

import json
import random
import torch
import numpy as np
from pathlib import Path
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

from train import (
    Config, LipROIExtractor, KoreanGraphemeProcessor,
    LipReadingModel, FinetuneTrainer, ModelEvaluator, collate_fn
)

def test_finetune_only():
    """κΈ°μ΅΄ μ‚¬μ „ν•™μµ κ°€μ¤‘μΉλ¥Ό μ‚¬μ©ν• λ³Έν•™μµ ν…μ¤νΈ"""
    print("π€ λ³Έν•™μµ ν…μ¤νΈ μ‹μ‘!")
    
    # μ„¤μ •
    config = Config()
    config.FINETUNE_EPOCHS = 5  # λ³Έν•™μµ μ—ν¬ν¬ μ
    config.BATCH_SIZE = 2
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"π”§ λ””λ°”μ΄μ¤: {device}")
    
    # 1λ‹¨κ³„: λ³Έν•™μµ λ°μ΄ν„° μ¤€λΉ„
    print("\nπ“ 1λ‹¨κ³„: λ³Έν•™μµ λ°μ΄ν„° μ¤€λΉ„ μ¤‘...")
    
    # TL48μ—μ„ λΌλ²¨λ§λ λ°μ΄ν„° μ‚¬μ©
    labeled_data_dir = Path("009.λ¦½λ¦¬λ”©(μ…λ¨μ–‘) μμ„±μΈμ‹ λ°μ΄ν„°/01.λ°μ΄ν„°/1.Training/μμ†_λΌλ²¨λ§λ°μ΄ν„°")
    tl48_label_dir = labeled_data_dir / "TL48" / "μ†μν™κ²½1" / "C(μΌλ°μΈ)" / "M(λ‚¨μ„±)" / "M(λ‚¨μ„±)_24"
    
    # μ²« λ²μ§Έ JSON νμΌ μ‚¬μ©
    json_files = list(tl48_label_dir.glob("*.json"))
    if not json_files:
        raise FileNotFoundError(f"λΌλ²¨λ§ λ°μ΄ν„°λ¥Ό μ°Ύμ„ μ μ—†μµλ‹λ‹¤: {tl48_label_dir}")
    
    json_path = json_files[0]
    # JSON νμΌλ…μ—μ„ _jasoλ¥Ό μ κ±°ν•κ³  .mp4 ν™•μ¥μ μ¶”κ°€
    video_filename = json_path.stem.replace('_jaso', '') + ".mp4"
    # μ›μ²λ°μ΄ν„° κ²½λ΅μ—μ„ ν•΄λ‹Ή λΉ„λ””μ¤ νμΌ μ°ΎκΈ°
    source_data_dir = Path("009.λ¦½λ¦¬λ”©(μ…λ¨μ–‘) μμ„±μΈμ‹ λ°μ΄ν„°/01.λ°μ΄ν„°/1.Training/μ›μ²λ°μ΄ν„°")
    ts48_source_dir = source_data_dir / "TS48" / "μ†μν™κ²½1" / "C(μΌλ°μΈ)" / "M(λ‚¨μ„±)" / "M(λ‚¨μ„±)_24"
    video_path = ts48_source_dir / video_filename
    
    if not video_path.exists():
        raise FileNotFoundError(f"λΉ„λ””μ¤ νμΌμ΄ μ—†μµλ‹λ‹¤: {video_path}")
    
    print(f"π“ λ³Έν•™μµ λ°μ΄ν„°:")
    print(f"   - λΉ„λ””μ¤: {video_path.name}")
    print(f"   - λΌλ²¨: {json_path.name}")
    
    # λ³Έν•™μµ λ°μ΄ν„°μ…‹ μƒμ„±
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if isinstance(data, list):
            annotations = data[0]['Sentence_info']
        else:
            annotations = data.get('Sentence_info', [])
        
        print(f"π“ JSON κµ¬μ΅°: {type(data)}, μ–΄λ…Έν…μ΄μ… μ: {len(annotations)}")
        
        finetune_dataset = CustomLabeledDataset(str(video_path), annotations)
        print(f"β… λ³Έν•™μµ λ°μ΄ν„°μ…‹ μƒμ„± μ™„λ£: {len(finetune_dataset)}κ° μƒν”")
    except Exception as e:
        print(f"β οΈ λ³Έν•™μµ λ°μ΄ν„°μ…‹ μƒμ„± μ‹¤ν¨: {e}")
        raise
    
    # λ°μ΄ν„° λ¶„ν•  (8:2)
    total_samples = len(finetune_dataset)
    train_size = int(total_samples * 0.8)
    val_size = total_samples - train_size
    
    indices = list(range(total_samples))
    random.shuffle(indices)
    
    train_indices = indices[:train_size]
    val_indices = indices[train_size:]
    
    train_dataset = torch.utils.data.Subset(finetune_dataset, train_indices)
    val_dataset = torch.utils.data.Subset(finetune_dataset, val_indices)
    
    print(f"π“ ν›λ ¨ λ°μ΄ν„°: {len(train_dataset)}κ° μƒν”")
    print(f"π“ κ²€μ¦ λ°μ΄ν„°: {len(val_dataset)}κ° μƒν”")
    
    # λ³Έν•™μµ λ°μ΄ν„° λ΅λ”
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=True,
        num_workers=0,
        collate_fn=collate_fn,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config.BATCH_SIZE,
        shuffle=False,
        num_workers=0,
        collate_fn=collate_fn,
        drop_last=True
    )
    
    # 2λ‹¨κ³„: λ³Έν•™μµ λ¨λΈ μƒμ„± λ° μ‚¬μ „ν•™μµ κ°€μ¤‘μΉ λ΅λ“
    print("\nπ― 2λ‹¨κ³„: λ³Έν•™μµ λ¨λΈ μ¤€λΉ„...")
    
    # λ³Έν•™μµ λ¨λΈ μƒμ„±
    finetune_model = LipReadingModel(vocab_size=config.VOCAB_SIZE).to(device)
    
    # κΈ°μ΅΄ μ‚¬μ „ν•™μµλ κ°€μ¤‘μΉ λ΅λ“
    pretrain_weight_path = Path("models/pretrain/pretrained_encoder.pt")
    
    if pretrain_weight_path.exists():
        try:
            # μ‚¬μ „ν•™μµ λ¨λΈμ μΈμ½”λ” λ¶€λ¶„λ§ λ΅λ“
            pretrained_state = torch.load(pretrain_weight_path, map_location='cpu')
            
            # Visual Frontend κ°€μ¤‘μΉ λ΅λ“
            if 'visual_frontend' in pretrained_state:
                finetune_model.visual_frontend.load_state_dict(pretrained_state['visual_frontend'])
                print("β… Visual frontend κ°€μ¤‘μΉ λ΅λ“ μ™„λ£")
            
            # Temporal Encoder κ°€μ¤‘μΉ λ΅λ“
            if 'temporal_encoder' in pretrained_state:
                finetune_model.temporal_encoder.load_state_dict(pretrained_state['temporal_encoder'])
                print("β… Temporal encoder κ°€μ¤‘μΉ λ΅λ“ μ™„λ£")
                
        except Exception as e:
            print(f"β οΈ μ‚¬μ „ν•™μµ κ°€μ¤‘μΉ λ΅λ“ μ‹¤ν¨: {e}")
            print("μ²μλ¶€ν„° ν›λ ¨μ„ μ‹μ‘ν•©λ‹λ‹¤.")
    else:
        print("β οΈ μ‚¬μ „ν•™μµ κ°€μ¤‘μΉ νμΌμ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤.")
        print("μ²μλ¶€ν„° ν›λ ¨μ„ μ‹μ‘ν•©λ‹λ‹¤.")
    
    # 3λ‹¨κ³„: λ³Έν•™μµ μ‹¤ν–‰
    print("\nπ― 3λ‹¨κ³„: λ³Έν•™μµ μ‹μ‘...")
    
    # λ³Έν•™μµ ν›λ ¨κΈ° μƒμ„±
    finetune_trainer = FinetuneTrainer(
        model=finetune_model,
        train_loader=train_loader,
        val_loader=val_loader,
        device=device
    )
    
    # λ³Έν•™μµ μ‹¤ν–‰
    finetune_trainer.train(epochs=config.FINETUNE_EPOCHS)
    
    # 4λ‹¨κ³„: λ¨λΈ ν‰κ°€
    print("\nπ“ 4λ‹¨κ³„: λ¨λΈ ν‰κ°€...")
    
    # ν‰κ°€κΈ° μƒμ„±
    evaluator = ModelEvaluator(finetune_model, device)
    
    # κ²€μ¦ λ°μ΄ν„°λ΅ ν‰κ°€
    metrics = evaluator.evaluate(val_loader)
    
    print(f"π“ ν‰κ°€ κ²°κ³Ό:")
    print(f"   - Grapheme Error Rate: {metrics['ger']:.4f}")
    print(f"   - Character Error Rate: {metrics['cer']:.4f}")
    
    # 5λ‹¨κ³„: λ¨λΈ μ €μ¥
    print("\nπ’Ύ 5λ‹¨κ³„: λ¨λΈ μ €μ¥...")
    
    save_path = Path("models/finetune") / "finetuned_model.pt"
    save_path.parent.mkdir(exist_ok=True)
    torch.save(finetune_model.state_dict(), save_path)
    print(f"π’Ύ λ³Έν•™μµ λ¨λΈ μ €μ¥: {save_path}")
    
    return metrics

class CustomLabeledDataset(Dataset):
    """λΌλ²¨λ§λ λ°μ΄ν„°μ…‹"""
    
    def __init__(self, video_path, annotations):
        self.video_path = video_path
        self.annotations = annotations
        self.lip_extractor = LipROIExtractor()
        self.grapheme_processor = KoreanGraphemeProcessor()
        
        print(f"π“ λΉ„λ””μ¤: {video_path}")
        print(f"π“ μ–΄λ…Έν…μ΄μ… μ: {len(annotations)}")
    
    def __len__(self):
        return len(self.annotations)
    
    def __getitem__(self, idx):
        annotation = self.annotations[idx]
        
        # μ‹κ°„ κµ¬κ°„ μ •λ³΄
        start_time = annotation['start_time']
        end_time = annotation['end_time']
        
        # μ…μ  μ‹ν€€μ¤ μ¶”μ¶
        lip_sequence = self.lip_extractor.extract_video_lip_sequence(
            self.video_path, start_time, end_time
        )
        
        if lip_sequence is None or len(lip_sequence) == 0:
            raise ValueError(f"μ…μ  μ‹ν€€μ¤λ¥Ό μ¶”μ¶ν•  μ μ—†μµλ‹λ‹¤: {self.video_path}")
        
        # ν”„λ μ„ κΈΈμ΄ λ§μ¶”κΈ° (ν¨λ”© λλ” μλ¥΄κΈ°)
        target_length = 64
        if len(lip_sequence) < target_length:
            # ν¨λ”©: λ§μ§€λ§‰ ν”„λ μ„μΌλ΅ μ±„μ°κΈ°
            padding_frames = [lip_sequence[-1]] * (target_length - len(lip_sequence))
            lip_sequence = np.concatenate([lip_sequence, padding_frames], axis=0)
        elif len(lip_sequence) > target_length:
            # μλ¥΄κΈ°: μ•λ¶€λ¶„ μ‚¬μ©
            lip_sequence = lip_sequence[:target_length]
        
        # ν…μ„λ΅ λ³€ν™ (VisualFrontendκ°€ κΈ°λ€ν•λ” ν•νƒ: [T, H, W, C])
        frames = torch.from_numpy(lip_sequence).float()
        
        # μμ† λΌλ²¨ μ²λ¦¬
        sentence_text = annotation['sentence_text']
        # μμ† λ°°μ—΄μ„ λ¬Έμμ—΄λ΅ κ²°ν•©
        text = ''.join(sentence_text)
        label = self.grapheme_processor.text_to_graphemes(text)
        label = torch.tensor(label, dtype=torch.long)
        
        return {
            'frames': frames,
            'graphemes': label,
            'frame_length': torch.tensor(len(lip_sequence), dtype=torch.long),
            'grapheme_length': torch.tensor(len(label), dtype=torch.long),
            'text': text
        }

def main():
    """λ©”μΈ ν•¨μ"""
    import argparse
    
    parser = argparse.ArgumentParser(description='λ³Έν•™μµ ν…μ¤νΈ')
    parser.add_argument('--epochs', type=int, default=5, help='λ³Έν•™μµ μ—ν¬ν¬ μ')
    args = parser.parse_args()
    
    # μ„¤μ • μ—…λ°μ΄νΈ
    config = Config()
    config.FINETUNE_EPOCHS = args.epochs
    
    # ν…μ¤νΈ μ‹¤ν–‰
    metrics = test_finetune_only()
    
    print("\nπ‰ λ³Έν•™μµ ν…μ¤νΈ μ™„λ£!")
    print(f"π“ μµμΆ… μ„±λ¥: GER={metrics['ger']:.4f}, CER={metrics['cer']:.4f}")

if __name__ == "__main__":
    main() 