# ì™„ì „í•œ í•œêµ­ì–´ ë¦½ë¦¬ë”© íŒŒì´í”„ë¼ì¸
# MP4 íŒŒì¼ë§Œ ìˆìœ¼ë©´ ì‚¬ì „í•™ìŠµë¶€í„° í‰ê°€ê¹Œì§€ ëª¨ë“  ê³¼ì • ìë™í™”

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader
import cv2
import mediapipe as mp
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
import json
import pickle
import time
import random
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')
import argparse

# ===========================================
# ì„¤ì • ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°
# ===========================================

class Config:
    # ì…ìˆ  ROI ì„¤ì •
    LIP_ROI_SIZE = (112, 112)  # ì…ìˆ  ì˜ì—­ í¬ê¸° (ì •ì‚¬ê°í˜•)
    LIP_MARGIN = 10           # ì…ìˆ  ì£¼ë³€ ì—¬ë°± (í”½ì…€) - ì‘ì€ ì˜ì—­
    
    # ì‹œí€€ìŠ¤ ì„¤ì •
    PRETRAIN_SEQUENCE_LENGTH = 16  # ì‚¬ì „í•™ìŠµ ì‹œí€€ìŠ¤ ê¸¸ì´ (0.64ì´ˆ @ 25fps)
    TARGET_FPS = 25               # í‘œì¤€ í”„ë ˆì„ë ˆì´íŠ¸
    
    # í›ˆë ¨ ì„¤ì •
    PRETRAIN_EPOCHS = 100
    FINETUNE_EPOCHS = 50
    BATCH_SIZE = 8
    LEARNING_RATE = 1e-4
    
    # ëª¨ë¸ ì„¤ì •
    D_MODEL = 512
    NHEAD = 8
    NUM_ENCODER_LAYERS = 6
    VOCAB_SIZE = 54  # 53ê°œ ìì†Œ + blank
    
    # ë°ì´í„° ê²½ë¡œ
    UNLABELED_VIDEOS_DIR = "data/unlabeled_videos"
    LABELED_VIDEO_PATH = "data/labeled_video.mp4"
    LABELS_JSON_PATH = "data/labels.json"
    PROCESSED_DIR = "data/processed"
    MODELS_DIR = "models"
    
    # GPU ì„¤ì •
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# ===========================================
# 1. ì…ìˆ  ROI ì¶”ì¶œê¸° (MediaPipe ê¸°ë°˜)
# ===========================================

class LipROIExtractor:
    """ì…ìˆ  ì˜ì—­ ì •ë°€ ì¶”ì¶œê¸° (MediaPipe ì‚¬ìš©)"""
    
    def __init__(self):
        # MediaPipe Face Mesh ì´ˆê¸°í™”
        self.mp_face_mesh = mp.solutions.face_mesh
        self.face_mesh = self.mp_face_mesh.FaceMesh(
            static_image_mode=False,
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5,  # ë” ì•ˆì •ì ì¸ ì„¤ì •
            min_tracking_confidence=0.5     # ë” ì•ˆì •ì ì¸ ì„¤ì •
        )
        
        # ì…ìˆ  ëœë“œë§ˆí¬ ì¸ë±ìŠ¤ (MediaPipe Face Mesh ê¸°ì¤€)
        # ì™¸ê³½ ì…ìˆ  (468ê°œ ëœë“œë§ˆí¬ ì¤‘)
        self.outer_lip_indices = [
            61, 84, 17, 314, 405, 320, 307, 375, 321, 308, 324, 318,
            78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308
        ]
        
        # ë‚´ê³½ ì…ìˆ 
        self.inner_lip_indices = [
            78, 191, 80, 81, 82, 13, 312, 311, 310, 415, 308, 324, 318
        ]
        
        self.all_lip_indices = self.outer_lip_indices + self.inner_lip_indices
        
        # ì´ì „ í”„ë ˆì„ ì €ì¥ (ë°±ì—…ìš©)
        self.last_valid_lip_region = None
        self.consecutive_failures = 0
        self.max_consecutive_failures = 10  # ì—°ì† ì‹¤íŒ¨ ì œí•œ
    
    def extract_lip_region(self, frame):
        """
        í”„ë ˆì„ì—ì„œ ì…ìˆ  ì˜ì—­ ì¶”ì¶œ (MediaPipe ì‚¬ìš©)
        
        Args:
            frame: BGR ì´ë¯¸ì§€ [H, W, 3]
            
        Returns:
            lip_region: ì…ìˆ  ì˜ì—­ [112, 112, 3] ë˜ëŠ” None
            landmarks: ì…ìˆ  ëœë“œë§ˆí¬ ì¢Œí‘œ
        """
        try:
            # RGBë¡œ ë³€í™˜ (MediaPipeëŠ” RGB ì‚¬ìš©)
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            
            # MediaPipeë¡œ ì–¼êµ´ ëœë“œë§ˆí¬ ê²€ì¶œ
            results = self.face_mesh.process(rgb_frame)
            
            if not results.multi_face_landmarks:
                return None, None
            
            # ì²« ë²ˆì§¸ ì–¼êµ´ ì„ íƒ
            face_landmarks = results.multi_face_landmarks[0]
            
            # ì…ìˆ  ëœë“œë§ˆí¬ ì¢Œí‘œ ì¶”ì¶œ
            lip_points = []
            for idx in self.all_lip_indices:
                landmark = face_landmarks.landmark[idx]
                # ì¢Œí‘œë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
                x = int(landmark.x * frame.shape[1])
                y = int(landmark.y * frame.shape[0])
                lip_points.append([x, y])
            
            lip_points = np.array(lip_points)
            
            # ì…ìˆ  ë°”ìš´ë”© ë°•ìŠ¤ ê³„ì‚°
            x_min, y_min = lip_points.min(axis=0)
            x_max, y_max = lip_points.max(axis=0)
            
            # ì—¬ë°± ì¶”ê°€ (ì…ìˆ  ì£¼ë³€ ì •ë³´ë„ í¬í•¨)
            margin = Config.LIP_MARGIN
            x_min = max(0, x_min - margin)
            y_min = max(0, y_min - margin)
            x_max = min(frame.shape[1], x_max + margin)
            y_max = min(frame.shape[0], y_max + margin)
            
            # ì •ì‚¬ê°í˜• ROI ìƒì„± (ê¸´ ìª½ì— ë§ì¶¤)
            width = x_max - x_min
            height = y_max - y_min
            size = max(width, height)
            
            # ì¤‘ì‹¬ì  ê³„ì‚°
            center_x = (x_min + x_max) // 2
            center_y = (y_min + y_max) // 2
            
            # ì •ì‚¬ê°í˜• ì¢Œí‘œ ê³„ì‚°
            half_size = size // 2
            x1 = max(0, center_x - half_size)
            y1 = max(0, center_y - half_size)
            x2 = min(frame.shape[1], center_x + half_size)
            y2 = min(frame.shape[0], center_y + half_size)
            
            # ROI ì¶”ì¶œ
            lip_roi = frame[y1:y2, x1:x2]
            
            # 112x112ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
            lip_region = cv2.resize(lip_roi, Config.LIP_ROI_SIZE)
            
            return lip_region, lip_points
            
        except Exception as e:
            print(f"ì…ìˆ  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None, None
            return None, None
    
    def extract_video_lip_sequence(self, video_path, start_time=None, end_time=None):
        """
        ë¹„ë””ì˜¤ì—ì„œ ì…ìˆ  ì‹œí€€ìŠ¤ ì¶”ì¶œ
        
        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            start_time: ì‹œì‘ ì‹œê°„ (ì´ˆ, Noneì´ë©´ ì²˜ìŒë¶€í„°)
            end_time: ë ì‹œê°„ (ì´ˆ, Noneì´ë©´ ëê¹Œì§€)
            
        Returns:
            lip_sequence: ì…ìˆ  ì‹œí€€ìŠ¤ [T, 112, 112, 3]
        """
        cap = cv2.VideoCapture(str(video_path))
        
        if not cap.isOpened():
            raise ValueError(f"ë¹„ë””ì˜¤ë¥¼ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # ì‹œì‘/ë í”„ë ˆì„ ê³„ì‚°
        if start_time is not None:
            start_frame = int(start_time * fps)
            cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
        else:
            start_frame = 0
            
        if end_time is not None:
            end_frame = int(end_time * fps)
        else:
            end_frame = total_frames
        
        lip_frames = []
        frame_count = start_frame
        
        video_path_obj = Path(video_path)
        print(f"ğŸ“¹ ë¹„ë””ì˜¤ ì²˜ë¦¬ ì¤‘: {video_path_obj.name}")
        print(f"   FPS: {fps}, ì´ í”„ë ˆì„: {total_frames}")
        
        pbar = tqdm(total=end_frame-start_frame, desc="í”„ë ˆì„ ì²˜ë¦¬")
        
        while frame_count < end_frame:
            ret, frame = cap.read()
            if not ret:
                break
            
            # ì…ìˆ  ì˜ì—­ ì¶”ì¶œ
            lip_region, landmarks = self.extract_lip_region(frame)
            
            if lip_region is not None:
                # ì •ê·œí™” [0, 255] â†’ [0, 1]
                lip_region = lip_region.astype(np.float32) / 255.0
                lip_frames.append(lip_region)
            else:
                # ì…ìˆ  ê²€ì¶œ ì‹¤íŒ¨ ì‹œ ì´ì „ í”„ë ˆì„ ë³µì‚¬ (ìˆë‹¤ë©´)
                if len(lip_frames) > 0:
                    lip_frames.append(lip_frames[-1].copy())
            
            frame_count += 1
            pbar.update(1)
        
        cap.release()
        pbar.close()
        
        if len(lip_frames) == 0:
            raise ValueError(f"ìœ íš¨í•œ ì…ìˆ  í”„ë ˆì„ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        
        print(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {len(lip_frames)}í”„ë ˆì„")
        return np.array(lip_frames)

# ===========================================
# 2. í•œêµ­ì–´ ìì†Œ ì²˜ë¦¬ê¸°
# ===========================================

class KoreanGraphemeProcessor:
    """í•œêµ­ì–´ ìì†Œ ë¶„í•´/ì¡°í•© ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        # í•œêµ­ì–´ ìì†Œ ë§¤í•‘ (ë…¼ë¬¸ ê¸°ì¤€ 54ê°œ í´ë˜ìŠ¤)
        self.grapheme_to_idx = {
            '<blank>': 0,  # CTC blank í† í°
            ' ': 1,        # ê³µë°±
            
            # ì´ˆì„± (19ê°œ)
            'ã„±': 2, 'ã„²': 3, 'ã„´': 4, 'ã„·': 5, 'ã„¸': 6, 'ã„¹': 7, 'ã…': 8, 'ã…‚': 9,
            'ã…ƒ': 10, 'ã……': 11, 'ã…†': 12, 'ã…‡': 13, 'ã…ˆ': 14, 'ã…‰': 15, 'ã…Š': 16,
            'ã…‹': 17, 'ã…Œ': 18, 'ã…': 19, 'ã…': 20,
            
            # ì¤‘ì„± (21ê°œ)
            'ã…': 21, 'ã…': 22, 'ã…‘': 23, 'ã…’': 24, 'ã…“': 25, 'ã…”': 26, 'ã…•': 27,
            'ã…–': 28, 'ã…—': 29, 'ã…˜': 30, 'ã…™': 31, 'ã…š': 32, 'ã…›': 33, 'ã…œ': 34,
            'ã…': 35, 'ã…': 36, 'ã…Ÿ': 37, 'ã… ': 38, 'ã…¡': 39, 'ã…¢': 40, 'ã…£': 41,
            
            # ì¢…ì„± (ê°„ì†Œí™”ëœ 12ê°œ)
            'ã„±_': 42, 'ã„´_': 43, 'ã„·_': 44, 'ã„¹_': 45, 'ã…_': 46, 'ã…‚_': 47,
            'ã……_': 48, 'ã…‡_': 49, 'ã…ˆ_': 50, 'ã…Š_': 51, 'ã…‹_': 52, 'ã…Œ_': 53
        }
        
        self.idx_to_grapheme = {v: k for k, v in self.grapheme_to_idx.items()}
        
    def text_to_graphemes(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ ìì†Œ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        graphemes = []
        
        for char in text:
            if char == ' ':
                graphemes.append(self.grapheme_to_idx[' '])
            elif 'ê°€' <= char <= 'í£':
                decomposed = self.decompose_hangul(char)
                for g in decomposed:
                    if g in self.grapheme_to_idx:
                        graphemes.append(self.grapheme_to_idx[g])
            # ê¸°íƒ€ ë¬¸ìëŠ” ë¬´ì‹œ
            
        return graphemes
    
    def decompose_hangul(self, char):
        """í•œê¸€ ìŒì ˆì„ ìì†Œë¡œ ë¶„í•´"""
        if not ('ê°€' <= char <= 'í£'):
            return []
        
        base = ord(char) - ord('ê°€')
        ì´ˆì„±_idx = base // (21 * 28)
        ì¤‘ì„±_idx = (base % (21 * 28)) // 28
        ì¢…ì„±_idx = base % 28
        
        ì´ˆì„±_list = ['ã„±', 'ã„²', 'ã„´', 'ã„·', 'ã„¸', 'ã„¹', 'ã…', 'ã…‚', 'ã…ƒ', 'ã……', 'ã…†', 'ã…‡', 'ã…ˆ', 'ã…‰', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…']
        ì¤‘ì„±_list = ['ã…', 'ã…', 'ã…‘', 'ã…’', 'ã…“', 'ã…”', 'ã…•', 'ã…–', 'ã…—', 'ã…˜', 'ã…™', 'ã…š', 'ã…›', 'ã…œ', 'ã…', 'ã…', 'ã…Ÿ', 'ã… ', 'ã…¡', 'ã…¢', 'ã…£']
        ì¢…ì„±_list = ['', 'ã„±', 'ã„²', 'ã„³', 'ã„´', 'ã„µ', 'ã„¶', 'ã„·', 'ã„¹', 'ã„º', 'ã„»', 'ã„¼', 'ã„½', 'ã„¾', 'ã„¿', 'ã…€', 'ã…', 'ã…‚', 'ã…„', 'ã……', 'ã…†', 'ã…‡', 'ã…ˆ', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…']
        
        result = [ì´ˆì„±_list[ì´ˆì„±_idx], ì¤‘ì„±_list[ì¤‘ì„±_idx]]
        
        if ì¢…ì„±_idx > 0:
            # ì¢…ì„±ì„ ê°„ì†Œí™” (ë³µì¡í•œ ì¢…ì„±ì€ ê¸°ë³¸ ììŒìœ¼ë¡œ)
            ì¢…ì„± = ì¢…ì„±_list[ì¢…ì„±_idx]
            if ì¢…ì„± in ['ã„±', 'ã„²', 'ã„³']:
                result.append('ã„±_')
            elif ì¢…ì„± in ['ã„´', 'ã„µ', 'ã„¶']:
                result.append('ã„´_')
            elif ì¢…ì„± == 'ã„·':
                result.append('ã„·_')
            elif ì¢…ì„± in ['ã„¹', 'ã„º', 'ã„»', 'ã„¼', 'ã„½', 'ã„¾', 'ã„¿', 'ã…€']:
                result.append('ã„¹_')
            elif ì¢…ì„± == 'ã…':
                result.append('ã…_')
            elif ì¢…ì„± in ['ã…‚', 'ã…„']:
                result.append('ã…‚_')
            elif ì¢…ì„± in ['ã……', 'ã…†']:
                result.append('ã……_')
            elif ì¢…ì„± == 'ã…‡':
                result.append('ã…‡_')
            elif ì¢…ì„± == 'ã…ˆ':
                result.append('ã…ˆ_')
            elif ì¢…ì„± == 'ã…Š':
                result.append('ã…Š_')
            elif ì¢…ì„± == 'ã…‹':
                result.append('ã…‹_')
            elif ì¢…ì„± == 'ã…Œ':
                result.append('ã…Œ_')
            
        return result
    
    def indices_to_text(self, indices):
        """ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (CTC ë””ì½”ë”©ìš©)"""
        graphemes = []
        prev_idx = None
        
        for idx in indices:
            # blank í† í° ì œê±°
            if idx == 0:
                prev_idx = None
                continue
                
            # ì—°ì†ëœ ê°™ì€ í† í° ì œê±° (CTC ê·œì¹™)
            if idx != prev_idx:
                graphemes.append(self.idx_to_grapheme[idx])
                
            prev_idx = idx
        
        # ìì†Œë¥¼ ìŒì ˆë¡œ ì¬ì¡°í•© (ê°„ë‹¨í•œ ë²„ì „)
        return ''.join(graphemes)

# ===========================================
# 3. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ë“¤
# ===========================================

class UnlabeledVideoDataset(Dataset):
    """ì‚¬ì „í•™ìŠµìš© ë¬´ë¼ë²¨ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹"""
    
    def __init__(self, video_dir, num_clips_per_video=50):
        self.video_files = list(Path(video_dir).glob("*.mp4"))
        self.num_clips_per_video = num_clips_per_video
        self.lip_extractor = LipROIExtractor()
        
        if len(self.video_files) == 0:
            print("âš ï¸ ë¹„ë””ì˜¤ íŒŒì¼ì´ ì—†ì–´ ë”ë¯¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            self.use_dummy = True
            self.dummy_size = 1000
        else:
            self.use_dummy = False
            print(f"ğŸ“ {len(self.video_files)}ê°œ ë¹„ë””ì˜¤ íŒŒì¼ ë°œê²¬")
    
    def __len__(self):
        if self.use_dummy:
            return self.dummy_size
        return len(self.video_files) * self.num_clips_per_video
    
    def __getitem__(self, idx):
        if self.use_dummy:
            # ë”ë¯¸ ë°ì´í„°
            sequence = np.random.rand(Config.PRETRAIN_SEQUENCE_LENGTH, 112, 112, 3).astype(np.float32)
            return torch.FloatTensor(sequence)
        
        # ì‹¤ì œ ë°ì´í„°
        video_idx = idx // self.num_clips_per_video
        video_file = self.video_files[video_idx]
        
        try:
            # ë¹„ë””ì˜¤ ì „ì²´ ê¸¸ì´ êµ¬í•˜ê¸°
            cap = cv2.VideoCapture(str(video_file))
            fps = cap.get(cv2.CAP_PROP_FPS)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = total_frames / fps
            cap.release()
            
            # ëœë¤ ì‹œì‘ ì‹œê°„ ì„ íƒ
            clip_duration = Config.PRETRAIN_SEQUENCE_LENGTH / Config.TARGET_FPS
            if duration > clip_duration:
                start_time = random.uniform(0, duration - clip_duration)
                end_time = start_time + clip_duration
            else:
                start_time = 0
                end_time = duration
            
            # ì…ìˆ  ì‹œí€€ìŠ¤ ì¶”ì¶œ
            lip_sequence = self.lip_extractor.extract_video_lip_sequence(
                video_file, start_time, end_time
            )
            
            # ì •í™•íˆ 16í”„ë ˆì„ìœ¼ë¡œ ë§ì¶”ê¸°
            if len(lip_sequence) >= Config.PRETRAIN_SEQUENCE_LENGTH:
                lip_sequence = lip_sequence[:Config.PRETRAIN_SEQUENCE_LENGTH]
            else:
                # ë¶€ì¡±í•˜ë©´ ë§ˆì§€ë§‰ í”„ë ˆì„ ë°˜ë³µ
                padding = np.tile(
                    lip_sequence[-1:], 
                    (Config.PRETRAIN_SEQUENCE_LENGTH - len(lip_sequence), 1, 1, 1)
                )
                lip_sequence = np.concatenate([lip_sequence, padding], axis=0)
            
            return torch.FloatTensor(lip_sequence)
            
        except Exception as e:
            print(f"ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ë°ì´í„° ë°˜í™˜
            sequence = np.random.rand(Config.PRETRAIN_SEQUENCE_LENGTH, 112, 112, 3).astype(np.float32)
            return torch.FloatTensor(sequence)

class LabeledVideoDataset(Dataset):
    """ë³¸í•™ìŠµìš© ë¼ë²¨ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹"""
    
    def __init__(self, video_path, json_path):
        self.video_path = Path(video_path)
        self.lip_extractor = LipROIExtractor()
        self.grapheme_processor = KoreanGraphemeProcessor()
        
        # ë¼ë²¨ ë¡œë“œ
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.annotations = data['Sentence_info']
        
        print(f"ğŸ“Š ë¼ë²¨ ë°ì´í„°: {len(self.annotations)}ê°œ ë¬¸ì¥")
    
    def __len__(self):
        return len(self.annotations)
    
    def __getitem__(self, idx):
        annotation = self.annotations[idx]
        
        # ì‹œê°„ êµ¬ê°„ ì •ë³´
        start_time = annotation['start_time']
        end_time = annotation['end_time']
        
        # ìì†Œ ë¼ë²¨ (ì´ë¯¸ ë¶„í•´ëœ ìƒíƒœ)
        sentence_graphemes = annotation['sentence_text']
        grapheme_indices = []
        
        for grapheme in sentence_graphemes:
            if grapheme in self.grapheme_processor.grapheme_to_idx:
                grapheme_indices.append(self.grapheme_processor.grapheme_to_idx[grapheme])
        
        # ì…ìˆ  ì‹œí€€ìŠ¤ ì¶”ì¶œ
        lip_sequence = self.lip_extractor.extract_video_lip_sequence(
            self.video_path, start_time, end_time
        )
        
        return {
            'frames': torch.FloatTensor(lip_sequence),
            'graphemes': torch.LongTensor(grapheme_indices),
            'frame_length': len(lip_sequence),
            'grapheme_length': len(grapheme_indices),
            'text': ''.join(sentence_graphemes)
        }

def collate_fn(batch):
    """ê°€ë³€ ê¸¸ì´ ë°°ì¹˜ ì²˜ë¦¬"""
    frames_list = [item['frames'] for item in batch]
    graphemes_list = [item['graphemes'] for item in batch]
    frame_lengths = [item['frame_length'] for item in batch]
    grapheme_lengths = [item['grapheme_length'] for item in batch]
    texts = [item['text'] for item in batch]
    
    # íŒ¨ë”©
    max_frame_len = max(frame_lengths)
    max_grapheme_len = max(grapheme_lengths)
    
    padded_frames = []
    padded_graphemes = []
    
    for i, frames in enumerate(frames_list):
        # í”„ë ˆì„ íŒ¨ë”©
        if len(frames) < max_frame_len:
            padding = torch.zeros(max_frame_len - len(frames), 112, 112, 3)
            frames = torch.cat([frames, padding], dim=0)
        padded_frames.append(frames)
        
        # ìì†Œ íŒ¨ë”©
        graphemes = graphemes_list[i]
        if len(graphemes) < max_grapheme_len:
            padding = torch.zeros(max_grapheme_len - len(graphemes), dtype=torch.long)
            graphemes = torch.cat([graphemes, padding], dim=0)
        padded_graphemes.append(graphemes)
    
    return {
        'frames': torch.stack(padded_frames),
        'graphemes': torch.stack(padded_graphemes),
        'frame_lengths': torch.LongTensor(frame_lengths),
        'grapheme_lengths': torch.LongTensor(grapheme_lengths),
        'texts': texts
    }

class CrawlerVideoDataset(Dataset):
    """í¬ë¡¤ëŸ¬ë¡œ ìƒì„±ëœ ë¹„ë¼ë²¨ë§ ë¹„ë””ì˜¤ ë°ì´í„°ì…‹"""
    
    def __init__(self, pickle_path):
        self.pickle_path = Path(pickle_path)
        
        # Pickle íŒŒì¼ ë¡œë“œ
        with open(self.pickle_path, 'rb') as f:
            self.sequences = pickle.load(f)
        
        print(f"ğŸ“Š í¬ë¡¤ëŸ¬ ë°ì´í„°: {len(self.sequences)}ê°œ ì‹œí€€ìŠ¤")
    
    def __len__(self):
        return len(self.sequences)
    
    def __getitem__(self, idx):
        sequence_data = self.sequences[idx]
        
        # ì‹œí€€ìŠ¤ ë°ì´í„° ì¶”ì¶œ
        lip_sequence = sequence_data['sequence']  # (16, 112, 112, 3)
        
        # ì •ê·œí™” (0-1 ë²”ìœ„)
        if lip_sequence.dtype == np.uint8:
            lip_sequence = lip_sequence.astype(np.float32) / 255.0
        
        # ì •í™•íˆ 16í”„ë ˆì„ìœ¼ë¡œ ë§ì¶”ê¸°
        if len(lip_sequence) >= Config.PRETRAIN_SEQUENCE_LENGTH:
            lip_sequence = lip_sequence[:Config.PRETRAIN_SEQUENCE_LENGTH]
        else:
            # ë¶€ì¡±í•˜ë©´ ë§ˆì§€ë§‰ í”„ë ˆì„ ë°˜ë³µ
            padding = np.tile(
                lip_sequence[-1:], 
                (Config.PRETRAIN_SEQUENCE_LENGTH - len(lip_sequence), 1, 1, 1)
            )
            lip_sequence = np.concatenate([lip_sequence, padding], axis=0)
        
        return torch.FloatTensor(lip_sequence)

# ===========================================
# 4. ëª¨ë¸ ì•„í‚¤í…ì²˜
# ===========================================

class VisualFrontend(nn.Module):
    """3D CNN + ResNet18 ê¸°ë°˜ ì‹œê°ì  íŠ¹ì§• ì¶”ì¶œê¸°"""
    
    def __init__(self):
        super().__init__()
        
        # 3D CNN for temporal features
        self.conv3d = nn.Sequential(
            nn.Conv3d(3, 64, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3)),
            nn.BatchNorm3d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1))
        )
        
        # ResNet18 backbone
        resnet = torchvision.models.resnet18(pretrained=True)
        self.resnet_layers = nn.Sequential(
            nn.Conv2d(64, 64, kernel_size=7, stride=2, padding=3, bias=False),
            resnet.bn1,
            resnet.relu,
            resnet.maxpool,
            resnet.layer1,
            resnet.layer2,
            resnet.layer3,
            resnet.layer4
        )
        
        self.global_pool = nn.AdaptiveAvgPool2d(1)
    
    def forward(self, x):
        """
        x: [B, T, H, W, C] = [batch, time, height, width, channels]
        return: [B, T, 512]
        """
        B, T, H, W, C = x.shape
        
        # 3D Conv ì ìš©
        x = x.permute(0, 4, 1, 2, 3)  # [B, C, T, H, W]
        x = self.conv3d(x)  # [B, 64, T, H', W']
        
        # ì‹œê°„ë³„ë¡œ ResNet ì ìš©
        x = x.permute(0, 2, 1, 3, 4)  # [B, T, 64, H', W']
        
        frame_features = []
        for t in range(T):
            frame = x[:, t]  # [B, 64, H', W']
            feat = self.resnet_layers(frame)  # [B, 512, 7, 7]
            feat = self.global_pool(feat)  # [B, 512, 1, 1]
            feat = feat.squeeze(-1).squeeze(-1)  # [B, 512]
            frame_features.append(feat)
        
        output = torch.stack(frame_features, dim=1)  # [B, T, 512]
        return output

class PositionalEncoding(nn.Module):
    """ìœ„ì¹˜ ì¸ì½”ë”©"""
    
    def __init__(self, d_model, max_len=500):  # 200 -> 500ìœ¼ë¡œ ì¦ê°€
        super().__init__()
        
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                           -(np.log(10000.0) / d_model))
        
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        self.register_buffer('pe', pe.unsqueeze(0))
    
    def forward(self, x):
        return x + self.pe[:, :x.size(1)]

class TemporalEncoder(nn.Module):
    """Transformer ê¸°ë°˜ ì‹œê°„ì  ì¸ì½”ë”"""
    
    def __init__(self, d_model=512, nhead=8, num_layers=6):
        super().__init__()
        
        self.pos_encoder = PositionalEncoding(d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=2048,
            dropout=0.1,
            activation='gelu',
            batch_first=True,
            norm_first=True
        )
        
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
    
    def forward(self, x, attention_mask=None):
        x = self.pos_encoder(x)
        x = self.transformer(x, src_key_padding_mask=attention_mask)
        return x

class MaskedVideoModel(nn.Module):
    """ì‚¬ì „í•™ìŠµìš© Masked Video Modeling"""
    
    def __init__(self):
        super().__init__()
        
        self.visual_frontend = VisualFrontend()
        self.temporal_encoder = TemporalEncoder()
        
        # ë””ì½”ë” (ë§ˆìŠ¤í‚¹ëœ í”„ë ˆì„ ë³µì›)
        self.decoder = nn.Sequential(
            nn.Linear(Config.D_MODEL, 1024),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(1024, 112 * 112 * 3)
        )
        
        self.mask_ratio = 0.15
    
    def create_mask(self, batch_size, seq_length, device):
        """ëœë¤ ë§ˆìŠ¤í‚¹ íŒ¨í„´ ìƒì„±"""
        num_masked = int(seq_length * self.mask_ratio)
        
        masks = []
        for _ in range(batch_size):
            mask = torch.zeros(seq_length, dtype=torch.bool)
            masked_indices = torch.randperm(seq_length)[:num_masked]
            mask[masked_indices] = True
            masks.append(mask)
        
        return torch.stack(masks).to(device)
    
    def forward(self, lip_frames):
        B, T, H, W, C = lip_frames.shape
        device = lip_frames.device
        
        # ë§ˆìŠ¤í‚¹ íŒ¨í„´ ìƒì„±
        mask = self.create_mask(B, T, device)
        
        # ë§ˆìŠ¤í‚¹ëœ í”„ë ˆì„ ìƒì„±
        masked_frames = lip_frames.clone()
        for b in range(B):
            masked_frames[b, mask[b]] = 0
        
        # ì¸ì½”ë” í†µê³¼
        visual_features = self.visual_frontend(masked_frames)
        temporal_features = self.temporal_encoder(visual_features)
        
        # ë§ˆìŠ¤í‚¹ëœ ë¶€ë¶„ë§Œ ë””ì½”ë”©í•˜ì—¬ ì†ì‹¤ ê³„ì‚°
        total_loss = 0
        num_masked_total = 0
        
        for b in range(B):
            if mask[b].sum() > 0:
                masked_features = temporal_features[b, mask[b]]
                reconstructed = self.decoder(masked_features)
                reconstructed = reconstructed.view(-1, H, W, C)
                
                original = lip_frames[b, mask[b]]
                loss = F.mse_loss(reconstructed, original)
                
                total_loss += loss
                num_masked_total += mask[b].sum().item()
        
        avg_loss = total_loss / B if B > 0 else 0
        return avg_loss

class LipReadingModel(nn.Module):
    """ë³¸í•™ìŠµìš© ë¦½ë¦¬ë”© ëª¨ë¸"""
    
    def __init__(self, vocab_size=54):
        super().__init__()
        
        self.visual_frontend = VisualFrontend()
        self.temporal_encoder = TemporalEncoder()
        
        # CTC Head
        self.ctc_head = nn.Sequential(
            nn.Linear(Config.D_MODEL, 256),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(256, vocab_size)
        )
    
    def forward(self, lip_frames, frame_lengths=None):
        B, T = lip_frames.shape[:2]
        
        # ì‹œê°ì  íŠ¹ì§• ì¶”ì¶œ
        visual_features = self.visual_frontend(lip_frames)
        
        # ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„± (íŒ¨ë”© ë¶€ë¶„ ë¬´ì‹œ)
        attention_mask = None
        if frame_lengths is not None:
            attention_mask = torch.zeros(B, T, dtype=torch.bool, device=lip_frames.device)
            for i, length in enumerate(frame_lengths):
                if length < T:
                    attention_mask[i, length:] = True
        
        # ì‹œê°„ì  ì¸ì½”ë”©
        temporal_features = self.temporal_encoder(visual_features, attention_mask)
        
        # CTC ì¶œë ¥
        logits = self.ctc_head(temporal_features)
        
        return logits
    
    def load_pretrained_weights(self, pretrained_path):
        """ì‚¬ì „í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ"""
        checkpoint = torch.load(pretrained_path, map_location='cpu')
        
        # ì¸ì½”ë” ë¶€ë¶„ë§Œ ë¡œë“œ
        if 'visual_frontend' in checkpoint:
            self.visual_frontend.load_state_dict(checkpoint['visual_frontend'])
            print("âœ… Visual frontend ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
        
        if 'temporal_encoder' in checkpoint:
            self.temporal_encoder.load_state_dict(checkpoint['temporal_encoder'])
            print("âœ… Temporal encoder ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")

# ===========================================
# 5. í›ˆë ¨ í´ë˜ìŠ¤ë“¤
# ===========================================

class PretrainTrainer:
    """ì‚¬ì „í•™ìŠµ íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, model, train_loader, device):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.device = device
        
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=Config.LEARNING_RATE,
            weight_decay=0.01
        )
        
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer, T_0=1000
        )
        
        self.train_losses = []
        
        # ì €ì¥ ë””ë ‰í† ë¦¬
        self.save_dir = Path(Config.MODELS_DIR) / 'pretrain'
        self.save_dir.mkdir(parents=True, exist_ok=True)
    
    def train_epoch(self):
        self.model.train()
        total_loss = 0
        
        pbar = tqdm(self.train_loader, desc="ì‚¬ì „í•™ìŠµ")
        
        for batch_idx, batch in enumerate(pbar):
            frames = batch.to(self.device)
            
            # ìˆœì „íŒŒ
            loss = self.model(frames)
            
            # ì—­ì „íŒŒ
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            self.scheduler.step()
            
            total_loss += loss.item()
            
            pbar.set_postfix({'Loss': f'{loss.item():.4f}'})
        
        return total_loss / len(self.train_loader)
    
    def train(self, epochs):
        print(f"ğŸš€ ì‚¬ì „í•™ìŠµ ì‹œì‘: {epochs} ì—í¬í¬")
        
        for epoch in range(epochs):
            print(f"\n=== Epoch {epoch+1}/{epochs} ===")
            
            train_loss = self.train_epoch()
            self.train_losses.append(train_loss)
            
            print(f"Train Loss: {train_loss:.4f}")
            
            # ëª¨ë¸ ì €ì¥ (10 ì—í¬í¬ë§ˆë‹¤)
            if (epoch + 1) % 10 == 0:
                self.save_checkpoint(epoch + 1, train_loss)
        
        # ìµœì¢… ì¸ì½”ë” ê°€ì¤‘ì¹˜ ì €ì¥
        self.save_encoder_weights()
        print("âœ… ì‚¬ì „í•™ìŠµ ì™„ë£Œ!")
    
    def save_checkpoint(self, epoch, loss):
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'loss': loss
        }
        torch.save(checkpoint, self.save_dir / f'checkpoint_epoch_{epoch}.pt')
        print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥: epoch {epoch}")
    
    def save_encoder_weights(self):
        """ì‚¬ì „í•™ìŠµëœ ì¸ì½”ë” ê°€ì¤‘ì¹˜ë§Œ ì €ì¥"""
        encoder_weights = {
            'visual_frontend': self.model.visual_frontend.state_dict(),
            'temporal_encoder': self.model.temporal_encoder.state_dict()
        }
        torch.save(encoder_weights, self.save_dir / 'pretrained_encoder.pt')
        print("âœ… ì¸ì½”ë” ê°€ì¤‘ì¹˜ ì €ì¥ ì™„ë£Œ")

class FinetuneTrainer:
    """ë³¸í•™ìŠµ íŠ¸ë ˆì´ë„ˆ"""
    
    def __init__(self, model, train_loader, val_loader, device):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=Config.LEARNING_RATE / 10,  # ì‚¬ì „í•™ìŠµë³´ë‹¤ ë‚®ì€ í•™ìŠµë¥ 
            weight_decay=0.01
        )
        
        self.scheduler = torch.optim.lr_scheduler.StepLR(
            self.optimizer, step_size=15, gamma=0.5
        )
        
        self.train_losses = []
        self.val_losses = []
        self.best_val_loss = float('inf')
        
        # ì €ì¥ ë””ë ‰í† ë¦¬
        self.save_dir = Path(Config.MODELS_DIR) / 'finetune'
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # í‰ê°€ ë©”íŠ¸ë¦­
        self.grapheme_processor = KoreanGraphemeProcessor()
    
    def compute_ctc_loss(self, logits, targets, input_lengths, target_lengths):
        """CTC ì†ì‹¤ ê³„ì‚°"""
        log_probs = F.log_softmax(logits.transpose(0, 1), dim=-1)
        
        loss = F.ctc_loss(
            log_probs=log_probs,
            targets=targets,
            input_lengths=input_lengths,
            target_lengths=target_lengths,
            blank=0,
            reduction='mean',
            zero_infinity=True
        )
        
        return loss
    
    def train_epoch(self):
        self.model.train()
        total_loss = 0
        
        pbar = tqdm(self.train_loader, desc="ë³¸í•™ìŠµ")
        
        for batch_idx, batch in enumerate(pbar):
            frames = batch['frames'].to(self.device)
            graphemes = batch['graphemes'].to(self.device)
            frame_lengths = batch['frame_lengths'].to(self.device)
            grapheme_lengths = batch['grapheme_lengths'].to(self.device)
            
            # ìˆœì „íŒŒ
            logits = self.model(frames, frame_lengths)
            
            # CTC ì†ì‹¤
            loss = self.compute_ctc_loss(logits, graphemes, frame_lengths, grapheme_lengths)
            
            # ì—­ì „íŒŒ
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            total_loss += loss.item()
            
            pbar.set_postfix({'Loss': f'{loss.item():.4f}'})
        
        return total_loss / len(self.train_loader)
    
    def validate(self):
        self.model.eval()
        total_loss = 0
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="ê²€ì¦"):
                frames = batch['frames'].to(self.device)
                graphemes = batch['graphemes'].to(self.device)
                frame_lengths = batch['frame_lengths'].to(self.device)
                grapheme_lengths = batch['grapheme_lengths'].to(self.device)
                
                logits = self.model(frames, frame_lengths)
                loss = self.compute_ctc_loss(logits, graphemes, frame_lengths, grapheme_lengths)
                
                total_loss += loss.item()
        
        return total_loss / len(self.val_loader)
    
    def train(self, epochs):
        print(f"ğŸ¯ ë³¸í•™ìŠµ ì‹œì‘: {epochs} ì—í¬í¬")
        
        for epoch in range(epochs):
            print(f"\n=== Epoch {epoch+1}/{epochs} ===")
            
            train_loss = self.train_epoch()
            val_loss = self.validate()
            
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            
            print(f"Train Loss: {train_loss:.4f}")
            print(f"Val Loss: {val_loss:.4f}")
            
            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
            self.scheduler.step()
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.save_best_model(epoch + 1, val_loss)
            
            # ì •ê¸° ì²´í¬í¬ì¸íŠ¸
            if (epoch + 1) % 10 == 0:
                self.save_checkpoint(epoch + 1, train_loss, val_loss)
        
        print("âœ… ë³¸í•™ìŠµ ì™„ë£Œ!")
    
    def save_checkpoint(self, epoch, train_loss, val_loss):
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'train_loss': train_loss,
            'val_loss': val_loss
        }
        torch.save(checkpoint, self.save_dir / f'checkpoint_epoch_{epoch}.pt')
    
    def save_best_model(self, epoch, val_loss):
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'val_loss': val_loss
        }
        torch.save(checkpoint, self.save_dir / 'best_model.pt')
        print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥: epoch {epoch}, val_loss {val_loss:.4f}")

# ===========================================
# 6. í‰ê°€ í´ë˜ìŠ¤
# ===========================================

class ModelEvaluator:
    """ëª¨ë¸ í‰ê°€ê¸°"""
    
    def __init__(self, model, device):
        self.model = model.to(device)
        self.device = device
        self.grapheme_processor = KoreanGraphemeProcessor()
    
    def ctc_decode(self, logits, frame_lengths):
        """CTC ë””ì½”ë”©"""
        predictions = torch.argmax(logits, dim=-1)  # [B, T]
        
        decoded_texts = []
        
        for batch_idx in range(predictions.shape[0]):
            pred_sequence = predictions[batch_idx, :frame_lengths[batch_idx]].cpu().numpy()
            
            # CTC ë””ì½”ë”©
            decoded_indices = []
            prev_token = None
            
            for token in pred_sequence:
                if token == 0:  # blank
                    prev_token = None
                    continue
                
                if token != prev_token:
                    decoded_indices.append(token)
                
                prev_token = token
            
            # ì¸ë±ìŠ¤ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            decoded_text = self.grapheme_processor.indices_to_text(decoded_indices)
            decoded_texts.append(decoded_text)
        
        return decoded_texts
    
    def compute_metrics(self, predictions, ground_truths):
        """í‰ê°€ ì§€í‘œ ê³„ì‚°"""
        total_ger = 0  # Grapheme Error Rate
        total_cer = 0  # Character Error Rate
        
        for pred, gt in zip(predictions, ground_truths):
            # ìì†Œ ë‹¨ìœ„ ì˜¤ë¥˜ìœ¨
            pred_graphemes = list(pred.replace(' ', ''))
            gt_graphemes = list(gt.replace(' ', ''))
            
            ger = self.edit_distance(pred_graphemes, gt_graphemes) / max(len(gt_graphemes), 1)
            total_ger += ger
            
            # ìŒì ˆ ë‹¨ìœ„ ì˜¤ë¥˜ìœ¨
            cer = self.edit_distance(list(pred), list(gt)) / max(len(gt), 1)
            total_cer += cer
        
        avg_ger = total_ger / len(predictions)
        avg_cer = total_cer / len(predictions)
        
        return avg_ger, avg_cer
    
    def edit_distance(self, seq1, seq2):
        """í¸ì§‘ ê±°ë¦¬ ê³„ì‚°"""
        m, n = len(seq1), len(seq2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        
        for i in range(m + 1):
            dp[i][0] = i
        for j in range(n + 1):
            dp[0][j] = j
        
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if seq1[i-1] == seq2[j-1]:
                    dp[i][j] = dp[i-1][j-1]
                else:
                    dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])
        
        return dp[m][n]
    
    def evaluate(self, dataloader):
        """ëª¨ë¸ í‰ê°€"""
        self.model.eval()
        
        all_predictions = []
        all_ground_truths = []
        
        print("ğŸ” ëª¨ë¸ í‰ê°€ ì¤‘...")
        
        with torch.no_grad():
            for batch in tqdm(dataloader, desc="í‰ê°€"):
                frames = batch['frames'].to(self.device)
                frame_lengths = batch['frame_lengths'].to(self.device)
                texts = batch['texts']
                
                # ì˜ˆì¸¡
                logits = self.model(frames, frame_lengths)
                predictions = self.ctc_decode(logits, frame_lengths)
                
                all_predictions.extend(predictions)
                all_ground_truths.extend(texts)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        ger, cer = self.compute_metrics(all_predictions, all_ground_truths)
        
        print(f"\nğŸ“Š í‰ê°€ ê²°ê³¼:")
        print(f"GER (ìì†Œ ì˜¤ë¥˜ìœ¨): {ger:.4f} ({ger*100:.2f}%)")
        print(f"CER (ìŒì ˆ ì˜¤ë¥˜ìœ¨): {cer:.4f} ({cer*100:.2f}%)")
        
        # ìƒ˜í”Œ ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“ ì˜ˆì¸¡ ìƒ˜í”Œ:")
        for i in range(min(5, len(all_predictions))):
            print(f"ì •ë‹µ: {all_ground_truths[i]}")
            print(f"ì˜ˆì¸¡: {all_predictions[i]}")
            print()
        
        return {
            'ger': ger,
            'cer': cer,
            'predictions': all_predictions,
            'ground_truths': all_ground_truths
        }

# ===========================================
# 7. ë©”ì¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ê¸°
# ===========================================

def setup_directories():
    """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
    dirs = [
        Config.UNLABELED_VIDEOS_DIR,
        Config.PROCESSED_DIR,
        Config.MODELS_DIR,
        "logs"
    ]
    
    for dir_path in dirs:
        Path(dir_path).mkdir(parents=True, exist_ok=True)

def pretrain_pipeline():
    """ì‚¬ì „í•™ìŠµ íŒŒì´í”„ë¼ì¸"""
    print("=" * 50)
    print("ğŸ¯ ì‚¬ì „í•™ìŠµ ì‹œì‘")
    print("=" * 50)
    
    # ë°ì´í„°ì…‹ ìƒì„±
    dataset = UnlabeledVideoDataset(Config.UNLABELED_VIDEOS_DIR)

    # === í¬ë¡­ëœ ì…ìˆ  ì´ë¯¸ì§€ 3ê°œ ì €ì¥ ===
    import cv2
    import os
    os.makedirs("cropped_samples", exist_ok=True)
    for i in range(3):
        lip_img = dataset[i]
        # torch.Tensorë¼ë©´ numpyë¡œ ë³€í™˜
        if hasattr(lip_img, "numpy"):
            lip_img = lip_img.numpy()
        # (ì±„ë„, H, W) â†’ (H, W, ì±„ë„) ë³€í™˜
        if len(lip_img.shape) == 3 and lip_img.shape[0] in [1, 3]:
            lip_img = lip_img.transpose(1, 2, 0)
        cv2.imwrite(f"cropped_samples/lip_sample_{i+1}.png", lip_img)
    # ===============================
    
    dataloader = DataLoader(
        dataset,
        batch_size=Config.BATCH_SIZE,
        shuffle=True,
        num_workers=0,  # Windowsì—ì„œ pickle ì˜¤ë¥˜ ë°©ì§€
        pin_memory=True
    )
    
    print(f"ğŸ“Š ì‚¬ì „í•™ìŠµ ë°ì´í„°: {len(dataset)}ê°œ í´ë¦½")
    
    # ëª¨ë¸ ë° íŠ¸ë ˆì´ë„ˆ ìƒì„±
    model = MaskedVideoModel()
    trainer = PretrainTrainer(model, dataloader, Config.DEVICE)
    
    # ëª¨ë¸ ì •ë³´
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ—ï¸ ëª¨ë¸ íŒŒë¼ë¯¸í„°: {total_params:,}ê°œ")
    
    # í›ˆë ¨ ì‹¤í–‰
    trainer.train(Config.PRETRAIN_EPOCHS)
    
    return trainer.save_dir / 'pretrained_encoder.pt'

def pretrain_with_crawler_data(pickle_path):
    """í¬ë¡¤ëŸ¬ ë°ì´í„°ë¡œ ì‚¬ì „í•™ìŠµ"""
    print("ğŸš€ í¬ë¡¤ëŸ¬ ë°ì´í„°ë¡œ ì‚¬ì „í•™ìŠµ ì‹œì‘")
    print("="*50)
    
    # ë°ì´í„°ì…‹ ìƒì„±
    dataset = CrawlerVideoDataset(pickle_path)
    dataloader = DataLoader(
        dataset, 
        batch_size=Config.BATCH_SIZE, 
        shuffle=True, 
        num_workers=0
    )
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    model = MaskedVideoModel().to(Config.DEVICE)
    
    # í›ˆë ¨ê¸° ì´ˆê¸°í™”
    trainer = PretrainTrainer(model, dataloader, Config.DEVICE)
    
    # í›ˆë ¨ ì‹¤í–‰
    trainer.train(Config.PRETRAIN_EPOCHS)
    
    print("âœ… í¬ë¡¤ëŸ¬ ë°ì´í„° ì‚¬ì „í•™ìŠµ ì™„ë£Œ!")
    return model

def finetune_pipeline(pretrained_weights_path):
    """ë³¸í•™ìŠµ íŒŒì´í”„ë¼ì¸"""
    print("=" * 50)
    print("ğŸ¯ ë³¸í•™ìŠµ ì‹œì‘")
    print("=" * 50)
    
    # ë°ì´í„°ì…‹ ìƒì„±
    dataset = LabeledVideoDataset(Config.LABELED_VIDEO_PATH, Config.LABELS_JSON_PATH)
    
    # í›ˆë ¨/ê²€ì¦ ë¶„í•  (8:2)
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    
    train_dataset, val_dataset = torch.utils.data.random_split(
        dataset, [train_size, val_size]
    )
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=Config.BATCH_SIZE // 2,  # ë³¸í•™ìŠµì€ ë°°ì¹˜ í¬ê¸° ì¤„ì„
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=0  # Windowsì—ì„œ pickle ì˜¤ë¥˜ ë°©ì§€
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=Config.BATCH_SIZE // 2,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=0  # Windowsì—ì„œ pickle ì˜¤ë¥˜ ë°©ì§€
    )
    
    print(f"ğŸ“Š ë³¸í•™ìŠµ ë°ì´í„°: í›ˆë ¨ {len(train_dataset)}ê°œ, ê²€ì¦ {len(val_dataset)}ê°œ")
    
    # ëª¨ë¸ ìƒì„± ë° ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ ë¡œë“œ
    model = LipReadingModel(Config.VOCAB_SIZE)
    model.load_pretrained_weights(pretrained_weights_path)
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í›ˆë ¨
    trainer = FinetuneTrainer(model, train_loader, val_loader, Config.DEVICE)
    trainer.train(Config.FINETUNE_EPOCHS)
    
    return trainer.save_dir / 'best_model.pt', val_loader

def evaluation_pipeline(model_path, val_loader):
    """í‰ê°€ íŒŒì´í”„ë¼ì¸"""
    print("=" * 50)
    print("ğŸ” ëª¨ë¸ í‰ê°€")
    print("=" * 50)
    
    # ëª¨ë¸ ë¡œë“œ
    model = LipReadingModel(Config.VOCAB_SIZE)
    checkpoint = torch.load(model_path, map_location=Config.DEVICE)
    model.load_state_dict(checkpoint['model_state_dict'])
    
    # í‰ê°€ ì‹¤í–‰
    evaluator = ModelEvaluator(model, Config.DEVICE)
    results = evaluator.evaluate(val_loader)
    
    # ê²°ê³¼ ì €ì¥
    with open('logs/evaluation_results.json', 'w', encoding='utf-8') as f:
        json.dump({
            'ger': results['ger'],
            'cer': results['cer'],
            'sample_predictions': results['predictions'][:10],
            'sample_ground_truths': results['ground_truths'][:10]
        }, f, ensure_ascii=False, indent=2)
    
    return results

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ë¦½ë¦¬ë”© ëª¨ë¸ í›ˆë ¨')
    parser.add_argument('--mode', choices=['pretrain', 'finetune', 'evaluate', 'crawler_pretrain'], 
                       default='pretrain', help='í›ˆë ¨ ëª¨ë“œ')
    parser.add_argument('--crawler-data', type=str, help='í¬ë¡¤ëŸ¬ ë°ì´í„° pickle íŒŒì¼ ê²½ë¡œ')
    parser.add_argument('--pretrained-weights', type=str, help='ì‚¬ì „í•™ìŠµëœ ê°€ì¤‘ì¹˜ ê²½ë¡œ')
    parser.add_argument('--model-path', type=str, help='í‰ê°€í•  ëª¨ë¸ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    # ë””ë ‰í† ë¦¬ ì„¤ì •
    setup_directories()
    
    if args.mode == 'crawler_pretrain':
        if not args.crawler_data:
            print("âŒ í¬ë¡¤ëŸ¬ ë°ì´í„° íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”: --crawler-data")
            return
        pretrain_with_crawler_data(args.crawler_data)
    
    elif args.mode == 'pretrain':
        pretrain_pipeline()
    
    elif args.mode == 'finetune':
        if not args.pretrained_weights:
            print("âŒ ì‚¬ì „í•™ìŠµëœ ê°€ì¤‘ì¹˜ ê²½ë¡œë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”: --pretrained-weights")
            return
        finetune_pipeline(args.pretrained_weights)
    
    elif args.mode == 'evaluate':
        if not args.model_path:
            print("âŒ í‰ê°€í•  ëª¨ë¸ ê²½ë¡œë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”: --model-path")
            return
        # í‰ê°€ íŒŒì´í”„ë¼ì¸ êµ¬í˜„ í•„ìš”
        print("í‰ê°€ ê¸°ëŠ¥ì€ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    else:
        print("âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë“œì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    main()